#include "ATen/Config.h"

#include "TH/TH.h"
#if AT_CUDA_ENABLED()
#undef THNN_
#include "THC/THC.h"
#endif
#include "ATen/Utils.h"
#include "ATen/CPUByteType.h"
#include "ATen/CPUByteTensor.h"
#include "ATen/CPUCharType.h"
#include "ATen/CPUCharTensor.h"
#include "ATen/CPUDoubleType.h"
#include "ATen/CPUDoubleTensor.h"
#include "ATen/CPUFloatType.h"
#include "ATen/CPUFloatTensor.h"
#include "ATen/CPUIntType.h"
#include "ATen/CPUIntTensor.h"
#include "ATen/CPULongType.h"
#include "ATen/CPULongTensor.h"
#include "ATen/CPUShortType.h"
#include "ATen/CPUShortTensor.h"
#include "ATen/CPUHalfType.h"
#include "ATen/CPUHalfTensor.h"
#include "ATen/SparseCPUByteType.h"
#include "ATen/SparseCPUByteTensor.h"
#include "ATen/SparseCPUCharType.h"
#include "ATen/SparseCPUCharTensor.h"
#include "ATen/SparseCPUDoubleType.h"
#include "ATen/SparseCPUDoubleTensor.h"
#include "ATen/SparseCPUFloatType.h"
#include "ATen/SparseCPUFloatTensor.h"
#include "ATen/SparseCPUIntType.h"
#include "ATen/SparseCPUIntTensor.h"
#include "ATen/SparseCPULongType.h"
#include "ATen/SparseCPULongTensor.h"
#include "ATen/SparseCPUShortType.h"
#include "ATen/SparseCPUShortTensor.h"
#include "ATen/CUDAByteType.h"
#include "ATen/CUDAByteTensor.h"
#include "ATen/CUDACharType.h"
#include "ATen/CUDACharTensor.h"
#include "ATen/CUDADoubleType.h"
#include "ATen/CUDADoubleTensor.h"
#include "ATen/CUDAFloatType.h"
#include "ATen/CUDAFloatTensor.h"
#include "ATen/CUDAIntType.h"
#include "ATen/CUDAIntTensor.h"
#include "ATen/CUDALongType.h"
#include "ATen/CUDALongTensor.h"
#include "ATen/CUDAShortType.h"
#include "ATen/CUDAShortTensor.h"
#include "ATen/CUDAHalfType.h"
#include "ATen/CUDAHalfTensor.h"
#include "ATen/SparseCUDAByteType.h"
#include "ATen/SparseCUDAByteTensor.h"
#include "ATen/SparseCUDACharType.h"
#include "ATen/SparseCUDACharTensor.h"
#include "ATen/SparseCUDADoubleType.h"
#include "ATen/SparseCUDADoubleTensor.h"
#include "ATen/SparseCUDAFloatType.h"
#include "ATen/SparseCUDAFloatTensor.h"
#include "ATen/SparseCUDAIntType.h"
#include "ATen/SparseCUDAIntTensor.h"
#include "ATen/SparseCUDALongType.h"
#include "ATen/SparseCUDALongTensor.h"
#include "ATen/SparseCUDAShortType.h"
#include "ATen/SparseCUDAShortTensor.h"

namespace at {

Tensor & CPUByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THByteTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THByteTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THByteTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THByteTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THByteTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THByteTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THByteTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        if (non_blocking) {
            THByteTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
            break;
        }
        THByteTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THByteTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THByteTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THByteTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THByteTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THByteTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THByteTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THByteTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUCharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUCharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCharTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCharTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCharTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCharTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCharTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCharTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCharTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCharTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCharTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        if (non_blocking) {
            THCharTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
            break;
        }
        THCharTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCharTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCharTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCharTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCharTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCharTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCharTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUDoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUDoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THDoubleTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THDoubleTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THDoubleTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THDoubleTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THDoubleTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THDoubleTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THDoubleTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THDoubleTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THDoubleTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        if (non_blocking) {
            THDoubleTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
            break;
        }
        THDoubleTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THDoubleTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THDoubleTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THDoubleTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THDoubleTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THDoubleTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THFloatTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THFloatTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THFloatTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THFloatTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THFloatTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THFloatTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THFloatTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THFloatTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THFloatTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THFloatTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        if (non_blocking) {
            THFloatTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
            break;
        }
        THFloatTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THFloatTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THFloatTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THFloatTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THFloatTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THIntTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THIntTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THIntTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THIntTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THIntTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THIntTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THIntTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THIntTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THIntTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THIntTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THIntTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THIntTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        if (non_blocking) {
            THIntTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
            break;
        }
        THIntTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THIntTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THIntTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THIntTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPULongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THLongTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THLongTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THLongTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THLongTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THLongTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THLongTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THLongTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THLongTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THLongTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THLongTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THLongTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THLongTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THLongTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        if (non_blocking) {
            THLongTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
            break;
        }
        THLongTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THLongTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THLongTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THShortTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THShortTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THShortTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THShortTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THShortTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THShortTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THShortTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THShortTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THShortTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THShortTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THShortTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THShortTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THShortTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THShortTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        if (non_blocking) {
            THShortTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
            break;
        }
        THShortTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THShortTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUHalfType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUHalfTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THHalfTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THHalfTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THHalfTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THHalfTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THHalfTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THHalfTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THHalfTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THHalfTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THHalfTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THHalfTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THHalfTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THHalfTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THHalfTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THHalfTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        if (non_blocking) {
            THHalfTensor_copyAsyncCuda(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
            break;
        }
        THHalfTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUCharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUCharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUDoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUDoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPULongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDAByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDAByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        if (non_blocking) {
            THCudaByteTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaByteTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaByteTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaByteTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaByteTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaByteTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaByteTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaByteTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaByteTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaByteTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaByteTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaByteTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaByteTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaByteTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaByteTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaByteTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDACharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDACharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaCharTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        if (non_blocking) {
            THCudaCharTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaCharTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaCharTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaCharTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaCharTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaCharTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaCharTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaCharTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaCharTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaCharTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaCharTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaCharTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaCharTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaCharTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaCharTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDADoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDADoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaDoubleTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaDoubleTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        if (non_blocking) {
            THCudaDoubleTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaDoubleTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaDoubleTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaDoubleTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaDoubleTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaDoubleTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaDoubleTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaDoubleTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaDoubleTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaDoubleTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaDoubleTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaDoubleTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaDoubleTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaDoubleTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDAFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDAFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        if (non_blocking) {
            THCudaTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDAIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDAIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaIntTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaIntTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaIntTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaIntTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        if (non_blocking) {
            THCudaIntTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaIntTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaIntTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaIntTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaIntTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaIntTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaIntTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaIntTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaIntTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaIntTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaIntTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaIntTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDALongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDALongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaLongTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaLongTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaLongTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaLongTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaLongTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        if (non_blocking) {
            THCudaLongTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaLongTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaLongTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaLongTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaLongTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaLongTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaLongTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaLongTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaLongTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaLongTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaLongTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDAShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDAShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaShortTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaShortTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaShortTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaShortTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaShortTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaShortTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        if (non_blocking) {
            THCudaShortTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaShortTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCudaShortTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaShortTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaShortTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaShortTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaShortTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaShortTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaShortTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaShortTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CUDAHalfType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CUDAHalfTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaHalfTensor_copyByte(context->thc_state, self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCudaHalfTensor_copyChar(context->thc_state, self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCudaHalfTensor_copyDouble(context->thc_state, self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCudaHalfTensor_copyFloat(context->thc_state, self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCudaHalfTensor_copyInt(context->thc_state, self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCudaHalfTensor_copyLong(context->thc_state, self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCudaHalfTensor_copyShort(context->thc_state, self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        if (non_blocking) {
            THCudaHalfTensor_copyAsyncCPU(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
            break;
        }
        THCudaHalfTensor_copyHalf(context->thc_state, self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAByte:
        THCudaHalfTensor_copyCudaByte(context->thc_state, self_->tensor, static_cast<CUDAByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAChar:
        THCudaHalfTensor_copyCudaChar(context->thc_state, self_->tensor, static_cast<CUDACharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDADouble:
        THCudaHalfTensor_copyCudaDouble(context->thc_state, self_->tensor, static_cast<CUDADoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAFloat:
        THCudaHalfTensor_copyCudaFloat(context->thc_state, self_->tensor, static_cast<CUDAFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAInt:
        THCudaHalfTensor_copyCudaInt(context->thc_state, self_->tensor, static_cast<CUDAIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDALong:
        THCudaHalfTensor_copyCudaLong(context->thc_state, self_->tensor, static_cast<CUDALongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAShort:
        THCudaHalfTensor_copyCudaShort(context->thc_state, self_->tensor, static_cast<CUDAShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaHalf(context->thc_state, self_->tensor, static_cast<CUDAHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDAByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDAByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDACharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDACharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDADoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDADoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDAFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDAFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDAIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDAIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDALongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDALongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCUDAShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCUDAShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}

}
