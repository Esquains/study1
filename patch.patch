diff --git a/torch/_dynamo/variables/higher_order_ops.py b/torch/_dynamo/variables/higher_order_ops.py
index 2292c71f048..5d7d87f810c 100644
--- a/torch/_dynamo/variables/higher_order_ops.py
+++ b/torch/_dynamo/variables/higher_order_ops.py
@@ -1088,7 +1088,7 @@ class AutogradFunctionMethodHigherOrderVariable(TorchHigherOrderOperatorVariable
         else:
             fn = TorchVariable(self.value)
         checkpoint = tx.copy_graphstate()
-        pre_guards = tx.output.guards
+        pre_guards = tx.output.guards.clone()
         graph_checkpoint = tx.output.graph
 
         # TODO: Support kwargs
diff --git a/torch/_guards.py b/torch/_guards.py
index e532a32cdd2..4f9e874476e 100644
--- a/torch/_guards.py
+++ b/torch/_guards.py
@@ -483,6 +483,8 @@ class GuardsSet:
         for o in others:
             for g in o:
                 self.add(g, skip=1)

+    def clone(self):
+        return GuardsSet(set(self.inner))
 
 
 class GuardsContext(Checkpointable[GuardsCheckpointState]):
