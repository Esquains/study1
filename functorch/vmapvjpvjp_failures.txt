============================= test session starts ==============================
platform linux -- Python 3.8.6, pytest-7.1.2, pluggy-0.13.1 -- /home/kshiteej/.conda/envs/pytorch-cuda-dev/bin/python3.8
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/kshiteej/Pytorch/pytorch/functorch/.hypothesis/examples')
rootdir: /home/kshiteej/Pytorch/pytorch/functorch/test, configfile: pytest.ini
plugins: profiling-1.7.0, hypothesis-6.12.0
collecting ... collected 14435 items / 13880 deselected / 555 selected

test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_H_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_T_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___getitem___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___getitem___functorch_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___radd___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rdiv___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rmatmul___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rmod___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rmul___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rpow___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rsub___cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_amax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_amin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_log_softmax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_mean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_normalize_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_prod_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_softmax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_softmin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_std_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_sum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_var_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_abs_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_acos_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_acosh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_add_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addbmm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addcdiv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addcmul_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addmm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addmm_decomposed_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addmv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_all_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_allclose_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_amax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_amin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_aminmax_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_angle_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_any_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_argmax_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_argmin_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_argsort_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_argwhere_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_as_strided_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_asin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_asinh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atan2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atan_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atanh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atleast_1d_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atleast_2d_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_atleast_3d_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_baddbmm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bernoulli_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bfloat16_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bfloat16_functorch_no_channels_last_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_block_diag_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bmm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bool_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bool_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_broadcast_tensors_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_broadcast_to_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_bucketize_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_byte_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_byte_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cartesian_prod_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cat_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cdist_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ceil_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_char_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_char_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cholesky_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cholesky_inverse_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cholesky_solve_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_chunk_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_clamp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_clone_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_column_stack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_combinations_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_complex_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_conj_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_conj_physical_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_contiguous_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_copysign_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_corrcoef_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cos_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cosh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_count_nonzero_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cov_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cross_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cummax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cummin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cumprod_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cumsum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cumulative_trapezoid_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_deg2rad_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diag_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diag_embed_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diagflat_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diagonal_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diagonal_scatter_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diff_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_digamma_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_dist_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_div_floor_rounding_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_div_no_rounding_mode_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_div_trunc_rounding_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_dot_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_double_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_double_functorch_no_channels_last_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_dsplit_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_dstack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_eig_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_einsum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_empty_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_eq_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_erf_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_erfc_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_erfinv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_exp2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_exp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_expand_as_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_expand_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_expm1_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_fft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_fft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_fftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_fftshift_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_hfft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_hfft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_hfftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ifft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ifft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ifftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ifftshift_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_irfft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_irfft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_irfftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_rfft2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_rfft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_rfftn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_flatten_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_flip_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fliplr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_flipud_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_float_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_float_functorch_no_channels_last_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_float_power_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_floor_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_floor_divide_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fmax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fmin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fmod_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_frac_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_frexp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_full_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_gather_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ge_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_geqrf_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_gradient_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_gt_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_half_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_half_functorch_no_channels_last_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_heaviside_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_histc_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_histogram_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_histogramdd_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_hsplit_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_hstack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_hypot_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_i0_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_igamma_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_igammac_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_add_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_copy_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_fill_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_put_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_put_functorch_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_select_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_inner_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_int_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_int_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_inverse_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isclose_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isfinite_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isin_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isinf_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isnan_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isneginf_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isposinf_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_isreal_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_istft_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_kron_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_kthvalue_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ldexp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_le_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lerp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lgamma_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cholesky_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cholesky_ex_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cond_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cross_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_det_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eig_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigvals_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigvalsh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_householder_product_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_inv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_inv_ex_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_grad_oriented_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lu_factor_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lu_factor_ex_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_matrix_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_matrix_power_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_matrix_rank_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_matrix_rank_hermitian_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_multi_dot_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_norm_subgradients_at_zero_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_pinv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_pinv_hermitian_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_pinv_singular_cpu_float32 SKIPPED (test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_qr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_slogdet_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_triangular_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_svd_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_svdvals_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_tensorinv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_tensorsolve_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_vector_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log10_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log1p_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log_softmax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_log_softmax_dtype_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logaddexp2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logaddexp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logcumsumexp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logdet_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logical_and_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logical_not_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logical_or_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logical_xor_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logit_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logsumexp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_long_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_long_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lt_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_unpack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mH_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mT_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_fill_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_fill_functorch_Scalar_only_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_scatter_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_select_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_matmul_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_matrix_exp_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_max_binary_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_max_reduction_no_dim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_max_reduction_with_dim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_maximum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_median_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_meshgrid_list_of_tensors_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_meshgrid_variadic_tensors_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_min_binary_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_min_reduction_no_dim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_min_reduction_with_dim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_minimum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mode_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_movedim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_msort_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mul_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_multinomial_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mv_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mvlgamma_mvlgamma_p_1_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mvlgamma_mvlgamma_p_3_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_mvlgamma_mvlgamma_p_5_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nan_to_num_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanmean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanmedian_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanquantile_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nansum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_narrow_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ne_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_neg_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_new_empty_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_new_full_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_new_ones_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_new_zeros_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nextafter_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_avg_pool1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_avg_pool2d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_avg_pool3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_max_pool1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_max_pool2d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_adaptive_max_pool3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_avg_pool1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_avg_pool2d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_avg_pool3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_batch_norm_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_bilinear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_binary_cross_entropy_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_binary_cross_entropy_with_logits_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_celu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_no_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_depthwise_with_bias_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_groups_with_bias_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_no_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_padding_no_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_padding_with_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_with_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_strided_padding_dilation_no_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_strided_padding_dilation_with_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_with_bias_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv_transpose1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv_transpose2d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv_transpose3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_cosine_embedding_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_cosine_similarity_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_cross_entropy_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_ctc_loss_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_dropout2d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_dropout_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_elu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_embedding_bag_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_embedding_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_embedding_functorch_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_feature_alpha_dropout_with_train_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_feature_alpha_dropout_without_train_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_fractional_max_pool2d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_fractional_max_pool3d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_gaussian_nll_loss_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_gelu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_glu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_grid_sample_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_group_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_hardshrink_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_hardsigmoid_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_hardswish_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_hardtanh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_hinge_embedding_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_huber_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_instance_norm_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_area_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_bicubic_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_bilinear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_linear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_nearest_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_interpolate_trilinear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_kl_div_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_l1_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_layer_norm_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_leaky_relu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_linear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_local_response_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_logsigmoid_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_margin_ranking_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool2d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool1d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool1d_grad_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_grad_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool3d_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool3d_grad_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_mish_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_mse_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_mse_loss_functorch_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_multi_margin_loss_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_multilabel_margin_loss_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_multilabel_soft_margin_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_nll_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_normalize_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pad_circular_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pad_constant_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pad_reflect_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pad_replicate_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pairwise_distance_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pdist_cpu_float32 SKIPPED (Skipped! Operation does not support gradgrad)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pixel_shuffle_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_pixel_unshuffle_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_poisson_nll_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_prelu_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_relu6_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_relu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_rrelu_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_selu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_silu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_smooth_l1_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_soft_margin_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_softmin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_softmin_with_dtype_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_softplus_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_softshrink_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_softsign_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_tanhshrink_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_threshold_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_triplet_margin_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_triplet_margin_with_distance_loss_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_unfold_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_upsample_bilinear_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_upsample_nearest_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nonzero_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_norm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_norm_fro_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_norm_inf_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_norm_nuc_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_normal_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_normal_number_mean_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ones_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ops_aten__new_zeros_with_same_feature_meta_functorchonly_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ops_aten_index_put_functorch_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ormqr_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_outer_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_pca_lowrank_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_permute_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_pinverse_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polar_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polygamma_polygamma_n_0_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polygamma_polygamma_n_1_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polygamma_polygamma_n_2_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polygamma_polygamma_n_3_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_polygamma_polygamma_n_4_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_positive_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_pow_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_prod_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_put_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_qr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_quantile_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_rad2deg_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_rand_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_randint_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_randn_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_ravel_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_real_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_reciprocal_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_remainder_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_renorm_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_repeat_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_repeat_interleave_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_reshape_as_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_reshape_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_resize__cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_resize_as__cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_resolve_conj_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_resolve_neg_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_roll_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_rot90_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_round_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_round_decimals_0_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_round_decimals_3_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_round_decimals_neg_3_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_rsqrt_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_rsub_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_add_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_amax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_amin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_mean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_prod_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_sum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_searchsorted_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_select_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_select_scatter_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sgn_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_short_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_short_functorch_no_channels_last_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sigmoid_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sign_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_signbit_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sin_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sinc_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sinh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_slice_scatter_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_softmax_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_softmax_with_dtype_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sort_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_entr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_erfcx_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_i0e_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_i1_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_i1e_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_log_ndtr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_ndtr_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_ndtri_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_polygamma_special_polygamma_n_0_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_xlog1py_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_zeta_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_split_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_split_list_args_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_split_with_sizes_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sqrt_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_square_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_squeeze_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_stack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_std_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_std_mean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_stft_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sub_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sum_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_sum_to_size_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_svd_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_svd_lowrank_cpu_float32 SKIPPED (Skipped!)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_symeig_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_t_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_along_dim_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tan_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tanh_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tensor_split_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tensordot_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tile_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_to_sparse_cpu_float32 XFAIL
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_topk_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_trace_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_transpose_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_trapezoid_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_trapz_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_triangular_solve_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_tril_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_triu_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_true_divide_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_trunc_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_unfold_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_unique_consecutive_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_unique_cpu_float32 SKIPPED (Skipped! Autograd not supported.)
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_unsqueeze_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_var_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_var_mean_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_vdot_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_view_as_complex_cpu_float32 FAILED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_view_as_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_view_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_vsplit_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_vstack_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_where_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_xlogy_cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_zero__cpu_float32 PASSED
test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_zeros_like_cpu_float32 SKIPPED (Skipped! Autograd not supported.)

=================================== FAILURES ===================================
___________ TestOperatorsCPU.test_vmapvjpvjp_linalg_eig_cpu_float32 ____________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_linalg_eig_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='linalg.eig', ref=None, aliases=(), variant_test_name='', op=<built-in function linalg_eig>, method_varian...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ..., 0.1781],
                     [0.1375, 0.1375, 0.1083, 0.6462, 0.6180]]], grad_fn=<AbsBackward0>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...      [-0.0015, -0.5042, -0.0892,  0.5436,  0.7815]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[ 1.5844, -0.4121, -0.6446...,
                 [-0.8330,  0.0382,  0.4593,  0.2841, -0.4384]]],
               grad_fn=<RepeatBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap over torch.allclose isn't supported yet. Please voice your support over at github.com/pytorch/functorch/issues/275

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_________ TestOperatorsCPU.test_vmapvjpvjp_linalg_eigvals_cpu_float32 __________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_linalg_eigvals_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='linalg.eigvals', ref=None, aliases=(), variant_test_name='', op=<built-in function linalg_eigvals>, metho...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[ 0.0118,  1.0068, -0.1539...,
                 [-0.4021,  0.5743, -0.5623, -0.0929, -0.1849]]],
               grad_fn=<SelectBackward0>)
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[-1.1150, -0.3856, -0.1023...1987+1.0407j, -0.5890-0.2914j, -0.8326+0.5133j,
                  0.3800+0.0270j]], grad_fn=<RepeatBackward0>)
    )
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=0, value=
    tensor([[[-0.6446,  0.4696,  1.1761, -0.1863, -0.2108],
             [-1.0341...       [ 0.1369, -0.0773,  0.1367, -0.5909,  0.1735],
             [ 0.4593,  0.2841, -0.4384, -1.1263,  1.5392]]])
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap over torch.allclose isn't supported yet. Please voice your support over at github.com/pytorch/functorch/issues/275

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
___ TestOperatorsCPU.test_vmapvjpvjp_linalg_householder_product_cpu_float32 ____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_linalg_householder_product_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='linalg.householder_product', ref=None, aliases=(<torch.testing._internal.opinfo.core.AliasInfo object at ...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...4738, -0.6296, -0.6233,  2.1830]]],
                   grad_fn=<LinalgHouseholderProductBackward0>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=2, value=
 ...rackingTensor(lvl=-2, value=
        tensor([ 0.9598, -0.1931, -0.0971,  1.1368, -1.3900], requires_grad=True)
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=2, value=
        tensor([[[ 1.1822,  1.1822],
      ....2209],
                 [-0.3929, -0.3929],
                 [-1.0080, -1.0080]]], grad_fn=<CloneBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over in a vmap. Please try to use out-of-place operators instead of inplace arithmetic. If said operator is being called inside the PyTorch framework, please file a bug report instead.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
___________ TestOperatorsCPU.test_vmapvjpvjp_linalg_svd_cpu_float32 ____________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_linalg_svd_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='linalg.svd', ref=None, aliases=(), variant_test_name='', op=<built-in function linalg_svd>, method_varian...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
            for loop_out, batched_out in generator:
>               self.assertEqual(loop_out, batched_out)
E               AssertionError: Tensor-likes are not close!
E               
E               Mismatched elements: 2 / 150 (1.3%)
E               Greatest absolute difference: 0.0008196830749511719 at index (1, 0, 4, 0) (up to 0.0001 allowed)
E               Greatest relative difference: 0.0001572327852315453 at index (1, 0, 3, 2) (up to 0.0001 allowed)
E               
E               The failure occurred for item [1]

test/test_ops.py:539: AssertionError
___________ TestOperatorsCPU.test_vmapvjpvjp_nanquantile_cpu_float32 ___________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nanquantile_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nanquantile', ref=None, aliases=(), variant_test_name='', op=<built-in method nanquantile of type object ...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = OpInfo(name='nanquantile', ref=None, aliases=(), variant_test_name='', op=<built-in method nanquantile of type object ...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)
args = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...         tensor([[0.0290, 0.4019],
                    [0.0290, 0.4019]], grad_fn=<RepeatBackward0>)
        )
    )
))
kwargs = {}

    def __call__(self, *args, **kwargs):
        """Calls the function variant of the operator."""
>       return self.op(*args, **kwargs)
E       RuntimeError: Batching rule not implemented for aten::equal. We could not generate a fallback.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: RuntimeError
____ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_batch_norm_cpu_float32 _____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_batch_norm_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.batch_norm', ref=None, aliases=(), variant_test_name='', op=<function batch_norm at 0x7fa9a...es=True, test_neg_view=True, assert_jit_shape_analysis=True, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...      [-7.8858,  5.7811, -2.7049, -1.8309,  4.2827]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
running_mean = tensor([ 0.6423,  1.0179, -0.8241,  1.4845, -0.4046])
running_var = tensor([29.2915, 23.5962, 20.4514, 24.6488, 19.1997])
weight = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
bias = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
training = True, momentum = 0.5, eps = 0.6

    def batch_norm(
        input: Tensor,
        running_mean: Optional[Tensor],
        running_var: Optional[Tensor],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        training: bool = False,
        momentum: float = 0.1,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Batch Normalization for each channel across a batch of data.
    
        See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,
        :class:`~torch.nn.BatchNorm3d` for details.
        """
        if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
            return handle_torch_function(
                batch_norm,
                (input, running_mean, running_var, weight, bias),
                input,
                running_mean,
                running_var,
                weight=weight,
                bias=bias,
                training=training,
                momentum=momentum,
                eps=eps,
            )
        if training:
            _verify_batch_size(input.size())
    
>       return torch.batch_norm(
            input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
        )
E       RuntimeError: Batch norm got a batched tensor as input while the running_mean or running_var, which will be updated in place, were not batched.
E       If you are using a module and do not need eval mode, please set `track_running_stats` to be False.If you are using a prebuilt module and do not need eval mode, please see the functorch website for resources on how to patch your module to work with vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:2444: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_binary_cross_entropy_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_binary_cross_entropy_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.binary_cross_entropy', ref=None, aliases=(), variant_test_name='', op=<function binary_cros...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([0.1312, 0.1312], grad_fn=<D...vl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([0.3473, 0.3473], grad_fn=<DivBackward1>)
    )
))
inputs = [GradTrackingTensor(lvl=-2, value=
    tensor(0.2634, requires_grad=True)
), GradTrackingTensor(lvl=-2, value=
    ten...=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([0.3378, 0.3378], grad_fn=<CloneBackward0>)
    )
)]
grad_outputs = (tensor(0.4039), BatchedTensor(lvl=1, bdim=0, value=
    tensor([-0.5909, -0.5909])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over in a vmap. Please try to use out-of-place operators instead of inplace arithmetic. If said operator is being called inside the PyTorch framework, please file a bug report instead.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
______ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_conv2d_cpu_float32 _______

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_conv2d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.conv2d', ref=None, aliases=(<torch.testing._internal.opinfo.core.AliasInfo object at 0x7fa8...es=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=True, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=1, value=
        tensor([[[[[  1.1519,   4.6830,   1...  8.8191,  -6.3608,  -6.6905,  -1.6700,   9.8936, -17.1593, -10.4415,
             -2.5627], grad_fn=<SumBackward2>)
))
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=4, value=
        tensor([[[[[ 2.7627e+00,  2.7627e+0...    [-0.0310, -0.1928, -0.5635, -1.6865],
              [ 0.4373, -0.5670,  0.2868,  0.5645]]]], requires_grad=True)
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=4, value=
    tensor([[[[[-0.0213, -0.0213],
               [ 0.1395,  0.1395],
           ...[ 1.2761,  1.2761],
            [-0.9684, -0.9684],
            [-0.2613, -0.2613],
            [-0.4532, -0.4532]])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: Given transposed=1, weight of size [4, 4, 3, 3], expected input[2, 8, 4, 4] to have 4 channels, but got 8 channels instead

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_conv2d_stride_depthwise_with_bias_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_conv2d_stride_depthwise_with_bias_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.conv2d', ref=None, aliases=(), variant_test_name='stride_depthwise_with_bias', op=<built-in...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=1, value=
        tensor([[[[[ 0.4816, -0.2758,  0.00...          3.4848, -0.7768, -4.7444, -0.3562,  2.7549, -0.7674, -7.0073, -5.3637],
           grad_fn=<SumBackward2>)
))
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=4, value=
        tensor([[[[[-0.9420, -0.9420],
    ...543e-01],
              [-1.0662e+00,  3.8607e-01],
              [-1.4982e+00, -4.4278e-01]]]], requires_grad=True)
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=4, value=
    tensor([[[[[-1.4040e+00, -1.4040e+00],
               [-6.4312e-02, -6.4312e-...[-0.5482, -0.5482],
            [-0.2751, -0.2751],
            [ 0.4486,  0.4486],
            [-0.2976, -0.2976]])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: Given transposed=1, weight of size [6, 48, 3, 2], expected input[2, 24, 3, 2] to have 6 channels, but got 24 channels instead

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_conv2d_stride_groups_with_bias_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_conv2d_stride_groups_with_bias_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.conv2d', ref=None, aliases=(), variant_test_name='stride_groups_with_bias', op=<built-in me...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=1, value=
        tensor([[[[[ 1.1357e+00, -1.1813e+0... tensor([-1.3181,  1.8217,  1.7548, -0.2442,  0.9412,  1.5456,  0.7393,  1.7579],
           grad_fn=<SumBackward2>)
))
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=4, value=
        tensor([[[[[-0.9420, -0.9420],
    ...     [[-0.2114,  0.4419],
              [ 0.2817,  0.1295],
              [-0.1432,  0.9835]]]], requires_grad=True)
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=4, value=
    tensor([[[[[ 0.3322,  0.3322],
               [-1.8122, -1.8122],
           ...[ 0.5164,  0.5164],
            [-0.4409, -0.4409],
            [-0.2386, -0.2386],
            [-0.0997, -0.0997]])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: Given transposed=1, weight of size [2, 48, 3, 2], expected input[2, 8, 3, 2] to have 2 channels, but got 8 channels instead

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_____ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_dropout2d_cpu_float32 _____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_dropout2d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.dropout2d', ref=None, aliases=(), variant_test_name='', op=<function <lambda> at 0x7fa8e1d3...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:16769: in <lambda>
    wrapper_set_seed(torch.nn.functional.dropout2d, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...      [-3.4979, -6.6321,  8.3980, -5.7677, -4.7705]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
p = 0.5, training = True, inplace = False

    def dropout2d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
        r"""
        Randomly zero out entire channels (a channel is a 2D feature map,
        e.g., the :math:`j`-th channel of the :math:`i`-th sample in the
        batched input is a 2D tensor :math:`\text{input}[i, j]`) of the input tensor).
        Each channel will be zeroed out independently on every forward call with
        probability :attr:`p` using samples from a Bernoulli distribution.
    
        See :class:`~torch.nn.Dropout2d` for details.
    
        Args:
            p: probability of a channel to be zeroed. Default: 0.5
            training: apply dropout if is ``True``. Default: ``True``
            inplace: If set to ``True``, will do this operation in-place. Default: ``False``
        """
        if has_torch_function_unary(input):
            return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)
        if p < 0.0 or p > 1.0:
            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
        inp_dim = input.dim()
        if inp_dim not in (3, 4):
            warn_msg = (f"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated "
                        "and will result in an error in a future release. To retain the behavior "
                        "and silence this warning, please use dropout instead. Note that dropout2d "
                        "exists to provide channel-wise dropout on inputs with 2 spatial dimensions, "
                        "a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).")
            warnings.warn(warn_msg)
    
        # TODO: Properly support no-batch-dim inputs. For now, these are NOT supported; passing
        # a 3D input will perform dropout1d behavior instead. This was done historically and the
        # behavior is maintained here for now.
        # See https://github.com/pytorch/pytorch/issues/77081
        if inp_dim == 3:
            warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
                          "1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C "
                          "is the channel dim. This behavior will change in a future release to interpret the "
                          "input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D "
                          "channel-wise dropout behavior, please switch to using dropout1d instead.")
    
>       result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
E       RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1344: RuntimeError
______ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_dropout_cpu_float32 ______

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_dropout_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.dropout', ref=None, aliases=(), variant_test_name='', op=<function <lambda> at 0x7fa8e1d3ca...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:16746: in <lambda>
    wrapper_set_seed(torch.nn.functional.dropout, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...       [ 4.3689, -8.2010,  6.4762, -1.7500,  6.6818]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
p = 0.5, training = True, inplace = False

    def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
        r"""
        During training, randomly zeroes some of the elements of the input
        tensor with probability :attr:`p` using samples from a Bernoulli
        distribution.
    
        See :class:`~torch.nn.Dropout` for details.
    
        Args:
            p: probability of an element to be zeroed. Default: 0.5
            training: apply dropout if is ``True``. Default: ``True``
            inplace: If set to ``True``, will do this operation in-place. Default: ``False``
        """
        if has_torch_function_unary(input):
            return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
        if p < 0.0 or p > 1.0:
            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
>       return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
E       RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1252: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_feature_alpha_dropout_with_train_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_feature_alpha_dropout_with_train_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.feature_alpha_dropout', ref=None, aliases=(), variant_test_name='with_train', op=<function ...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:16807: in <lambda>
    wrapper_set_seed(torch.nn.functional.feature_alpha_dropout, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...3.6276e+00, -6.9550e+00, -8.5880e+00, -5.2066e+00]]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
p = 0.5, training = True, inplace = False

    def feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:
        r"""
        Randomly masks out entire channels (a channel is a feature map,
        e.g. the :math:`j`-th channel of the :math:`i`-th sample in the batch input
        is a tensor :math:`\text{input}[i, j]`) of the input tensor). Instead of
        setting activations to zero, as in regular Dropout, the activations are set
        to the negative saturation value of the SELU activation function.
    
        Each element will be masked independently on every forward call with
        probability :attr:`p` using samples from a Bernoulli distribution.
        The elements to be masked are randomized on every forward call, and scaled
        and shifted to maintain zero mean and unit variance.
    
        See :class:`~torch.nn.FeatureAlphaDropout` for details.
    
        Args:
            p: dropout probability of a channel to be zeroed. Default: 0.5
            training: apply dropout if is ``True``. Default: ``True``
            inplace: If set to ``True``, will do this operation in-place. Default: ``False``
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                feature_alpha_dropout, (input,), input, p=p, training=training, inplace=inplace
            )
        if p < 0.0 or p > 1.0:
            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
>       return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
E       RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1414: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_fractional_max_pool2d_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_fractional_max_pool2d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.fractional_max_pool2d', ref=None, aliases=(), variant_test_name='', op=<function <lambda> a...=True, test_neg_view=False, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:12585: in <lambda>
    wrapper_set_seed(torch.nn.functional.fractional_max_pool2d, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/_jit_internal.py:485: in fn
    return if_false(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:503: in _fractional_max_pool2d
    return fractional_max_pool2d_with_indices(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...  6.6818, -5.2816, -8.5578,
                        -5.9662, -1.3953]]]]], grad_fn=<RepeatBackward0>)
        )
    )
)
kernel_size = 3, output_size = 2, output_ratio = None, return_indices = False
_random_samples = None

    def fractional_max_pool2d_with_indices(
        input: Tensor, kernel_size: BroadcastingList2[int],
        output_size: Optional[BroadcastingList2[int]] = None,
        output_ratio: Optional[BroadcastingList2[float]] = None,
        return_indices: bool = False,
        _random_samples: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        r"""Applies 2D fractional max pooling over an input signal composed of several input planes.
    
        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
    
        The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic
        step size determined by the target output size.
        The number of output features is equal to the number of input planes.
    
        Args:
            kernel_size: the size of the window to take a max over.
                         Can be a single number :math:`k` (for a square kernel of :math:`k \times k`)
                         or a tuple `(kH, kW)`
            output_size: the target output size of the image of the form :math:`oH \times oW`.
                         Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH`
            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
                          This has to be a number or tuple in the range (0, 1)
            return_indices: if ``True``, will return the indices along with the outputs.
                            Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.
    
        Examples::
            >>> input = torch.randn(20, 16, 50, 32)
            >>> # pool of square window of size=3, and target output size 13x12
            >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))
            >>> # pool of square window and target output size being half of input image size
            >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))
    
        .. _Fractional MaxPooling:
            http://arxiv.org/abs/1412.6071
        """
        if has_torch_function_variadic(input, _random_samples):
            return handle_torch_function(
                fractional_max_pool2d_with_indices,
                (input, _random_samples),
                input,
                kernel_size,
                output_size=output_size,
                output_ratio=output_ratio,
                return_indices=return_indices,
                _random_samples=_random_samples,
            )
        if output_size is None and output_ratio is None:
            raise ValueError("fractional_max_pool2d requires specifying either " "an output_size or an output_ratio")
        if output_size is None:
            assert output_ratio is not None
            _output_ratio = _pair(output_ratio)
            output_size = [int(input.size(-2) * _output_ratio[0]), int(input.size(-1) * _output_ratio[1])]
    
        if _random_samples is None:
            n_batch = 1 if input.dim() == 3 else input.size(0)
>           _random_samples = torch.rand(n_batch, input.size(-3), 2, dtype=input.dtype, device=input.device)
E           RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:481: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_fractional_max_pool3d_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_fractional_max_pool3d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.fractional_max_pool3d', ref=None, aliases=(), variant_test_name='', op=<function <lambda> a...=True, test_neg_view=False, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:12604: in <lambda>
    wrapper_set_seed(torch.nn.functional.fractional_max_pool3d, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/_jit_internal.py:485: in fn
    return if_false(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:608: in _fractional_max_pool3d
    return fractional_max_pool3d_with_indices(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ....6683e+00,  1.3732e+00, -4.3894e+00,  5.3084e+00]]]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
kernel_size = (2, 2, 2), output_size = 2, output_ratio = None
return_indices = False, _random_samples = None

    def fractional_max_pool3d_with_indices(
        input: Tensor, kernel_size: BroadcastingList3[int],
        output_size: Optional[BroadcastingList3[int]] = None,
        output_ratio: Optional[BroadcastingList3[float]] = None,
        return_indices: bool = False,
        _random_samples: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        r"""Applies 3D fractional max pooling over an input signal composed of several input planes.
    
        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
    
        The max-pooling operation is applied in :math:`kT \times kH \times kW` regions by a stochastic
        step size determined by the target output size.
        The number of output features is equal to the number of input planes.
    
        Args:
            kernel_size: the size of the window to take a max over.
                         Can be a single number :math:`k` (for a square kernel of :math:`k \times k \times k`)
                         or a tuple `(kT, kH, kW)`
            output_size: the target output size of the form :math:`oT \times oH \times oW`.
                         Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output
                         :math:`oH \times oH \times oH`
            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
                          This has to be a number or tuple in the range (0, 1)
            return_indices: if ``True``, will return the indices along with the outputs.
                            Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.
    
        Shape:
            - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.
            - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where
              :math:`(T_{out}, H_{out}, W_{out})=\text{output\_size}` or
              :math:`(T_{out}, H_{out}, W_{out})=\text{output\_ratio} \times (T_{in}, H_{in}, W_{in})`
    
        Examples::
            >>> input = torch.randn(20, 16, 50, 32, 16)
            >>> # pool of cubic window of size=3, and target output size 13x12x11
            >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))
            >>> # pool of cubic window and target output size being half of input size
            >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))
    
        .. _Fractional MaxPooling:
            http://arxiv.org/abs/1412.6071
        """
        if has_torch_function_variadic(input, _random_samples):
            return handle_torch_function(
                fractional_max_pool3d_with_indices,
                (input, _random_samples),
                input,
                kernel_size,
                output_size=output_size,
                output_ratio=output_ratio,
                return_indices=return_indices,
                _random_samples=_random_samples,
            )
        if output_size is None and output_ratio is None:
            raise ValueError("fractional_max_pool3d requires specifying either " "an output_size or an output_ratio")
        if output_size is None:
            assert output_ratio is not None
            _output_ratio = _triple(output_ratio)
            output_size = [
                int(input.size(-3) * _output_ratio[0]),
                int(input.size(-2) * _output_ratio[1]),
                int(input.size(-1) * _output_ratio[2]),
            ]
    
        if _random_samples is None:
            n_batch = 1 if input.dim() == 4 else input.size(0)
>           _random_samples = torch.rand(n_batch, input.size(-4), 3, dtype=input.dtype, device=input.device)
E           RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:586: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_gaussian_nll_loss_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_gaussian_nll_loss_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.gaussian_nll_loss', ref=None, aliases=(), variant_test_name='', op=<function gaussian_nll_l...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
target = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
var = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...1, 2.5856],
                    [7.0750, 5.1113, 6.9962, 7.4051, 2.5856]], grad_fn=<RepeatBackward0>)
        )
    )
)
full = False, eps = 1e-06, reduction = 'none'

    def gaussian_nll_loss(
        input: Tensor,
        target: Tensor,
        var: Tensor,
        full: bool = False,
        eps: float = 1e-6,
        reduction: str = "mean",
    ) -> Tensor:
        r"""Gaussian negative log likelihood loss.
    
        See :class:`~torch.nn.GaussianNLLLoss` for details.
    
        Args:
            input: expectation of the Gaussian distribution.
            target: sample from the Gaussian distribution.
            var: tensor of positive variance(s), one for each of the expectations
                in the input (heteroscedastic), or a single one (homoscedastic).
            full (bool, optional): include the constant term in the loss calculation. Default: ``False``.
            eps (float, optional): value added to var, for stability. Default: 1e-6.
            reduction (str, optional): specifies the reduction to apply to the output:
                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
                ``'mean'``: the output is the average of all batch member losses,
                ``'sum'``: the output is the sum of all batch member losses.
                Default: ``'mean'``.
        """
        if has_torch_function_variadic(input, target, var):
            return handle_torch_function(
                gaussian_nll_loss,
                (input, target, var),
                input,
                target,
                var,
                full=full,
                eps=eps,
                reduction=reduction,
            )
    
        # Check var size
        # If var.size == input.size, the case is heteroscedastic and no further checks are needed.
        # Otherwise:
        if var.size() != input.size():
    
            # If var is one dimension short of input, but the sizes match otherwise, then this is a homoscedastic case.
            # e.g. input.size = (10, 2, 3), var.size = (10, 2)
            # -> unsqueeze var so that var.shape = (10, 2, 1)
            # this is done so that broadcasting can happen in the loss calculation
            if input.size()[:-1] == var.size():
                var = torch.unsqueeze(var, -1)
    
            # This checks if the sizes match up to the final dimension, and the final dimension of var is of size 1.
            # This is also a homoscedastic case.
            # e.g. input.size = (10, 2, 3), var.size = (10, 2, 1)
            elif input.size()[:-1] == var.size()[:-1] and var.size(-1) == 1:  # Heteroscedastic case
                pass
    
            # If none of the above pass, then the size of var is incorrect.
            else:
                raise ValueError("var is of incorrect size")
    
        # Check validity of reduction mode
        if reduction != 'none' and reduction != 'mean' and reduction != 'sum':
            raise ValueError(reduction + " is not valid")
    
        # Entries of var must be non-negative
>       if torch.any(var < 0):
E       RuntimeError: vmap: It looks like you're either (1) calling .item() on a Tensor or (2) attempting to use a Tensor in some data-dependent control flow or (3) encountering this error in PyTorch internals. For (1): we don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. For (2): If you're doing some control flow instead, we don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 . For (3): please file an issue.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:2827: RuntimeError
___ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_instance_norm_cpu_float32 ___

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_instance_norm_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.instance_norm', ref=None, aliases=(), variant_test_name='', op=<function instance_norm at 0...es=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=True, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ...      [-7.8858,  5.7811, -2.7049, -1.8309,  4.2827]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)
running_mean = tensor([ 0.6423,  1.0179, -0.8241,  1.4845, -0.4046])
running_var = tensor([34.1274, 19.0229, 22.0926, 27.8971, 21.0066])
weight = tensor([-8.4784, -1.7658, -4.3228, -2.4005, -7.9506], requires_grad=True)
bias = tensor([ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337], requires_grad=True)
use_input_stats = True, momentum = 0.5, eps = 0.6

    def instance_norm(
        input: Tensor,
        running_mean: Optional[Tensor] = None,
        running_var: Optional[Tensor] = None,
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        use_input_stats: bool = True,
        momentum: float = 0.1,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Instance Normalization for each channel in each data sample in a
        batch.
    
        See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,
        :class:`~torch.nn.InstanceNorm3d` for details.
        """
        if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
            return handle_torch_function(
                instance_norm,
                (input, running_mean, running_var, weight, bias),
                input,
                running_mean=running_mean,
                running_var=running_var,
                weight=weight,
                bias=bias,
                use_input_stats=use_input_stats,
                momentum=momentum,
                eps=eps,
            )
        if use_input_stats:
            _verify_spatial_size(input.size())
>       return torch.instance_norm(
            input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled
        )
E       RuntimeError: Batch norm got a batched tensor as input while the running_mean or running_var, which will be updated in place, were not batched.
E       If you are using a module and do not need eval mode, please set `track_running_stats` to be False.If you are using a prebuilt module and do not need eval mode, please see the functorch website for resources on how to patch your module to work with vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:2489: RuntimeError
____ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_layer_norm_cpu_float32 _____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_layer_norm_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.layer_norm', ref=<function reference_layer_norm at 0x7fa8e219c5e0>, aliases=(<torch.testing...les=True, test_neg_view=True, assert_jit_shape_analysis=True, supports_expanded_weight=True, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[[-2.3040,  1.4317, -1.451...840],
                  [-0.6039, -0.6039],
                  [-0.5240, -0.5240]]]], grad_fn=<CloneBackward0>)
    )
))
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=3, value=
        tensor([[[[ 4.9473,  4.9473],
     ...840],
                  [-0.6039, -0.6039],
                  [-0.5240, -0.5240]]]], grad_fn=<CloneBackward0>)
    )
)]
grad_outputs = (tensor([[[-0.4175,  0.7618,  0.5356],
         [ 1.5739, -0.4864, -0.6622]]]), BatchedTensor(lvl=1, bdim=3, value=
  ...253]],
    
             [[ 0.5689,  0.5689],
              [-0.3702, -0.3702],
              [ 1.7509,  1.7509]]]])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over in a vmap. Please try to use out-of-place operators instead of inplace arithmetic. If said operator is being called inside the PyTorch framework, please file a bug report instead.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
____ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_max_pool2d_cpu_float32 _____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_max_pool2d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.max_pool2d', ref=None, aliases=(), variant_test_name='', op=<function boolean_dispatch.<loc...es=True, test_neg_view=True, assert_jit_shape_analysis=True, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[[[ 0.0354,  0.0000,  0.00...  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5516]]]]],
               grad_fn=<ReshapeAliasBackward0>)
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    tensor([[[[ 4.8620,  1.9829, -5.9219,  3.7416,  0.3313,  4.1063],
             ...],
                   [ 0.5892,  0.5892],
                   [ 0.5516,  0.5516]]]]], grad_fn=<CloneBackward0>)
    )
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=4, value=
    tensor([[[[[ 0.5875,  0.5875],
               [-0.6895, -0.6895],
           ...1740],
               [ 0.1127,  0.1127],
               [ 1.2150,  1.2150],
               [ 0.0245,  0.0245]]]]])
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: NYI: querying is_contiguous inside of vmap for memory_format other than torch.contiguous_format

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
___ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_max_unpool2d_cpu_float32 ____

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_max_unpool2d_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.max_unpool2d', ref=None, aliases=(), variant_test_name='', op=<function max_unpool2d at 0x7...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...  [0.0000, 0.0000, 8.1905, 0.0000, 0.0000, 0.0000]]]]],
                   grad_fn=<ViewBackward0>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...5161]],
            
                      [[8.1905, 8.1905, 7.0753]]]]], grad_fn=<RepeatBackward0>)
        )
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[[[-1.6510, -2.0284,  0.05...        [ 1.3835, -1.3065,  0.9416,  0.2277, -0.1658, -1.0559]]]]],
               grad_fn=<RepeatBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: NYI: Tensor.clone(memory_format) inside vmap is only supported with memory_format torch.preserve_format or torch.contiguous_format (got ChannelsLast)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_max_unpool2d_grad_cpu_float32 _

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_max_unpool2d_grad_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='nn.functional.max_unpool2d', ref=None, aliases=(), variant_test_name='grad', op=<function max_unpool2d at...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...  [8.2731, 0.0000, 0.0000, 7.0212, 0.0000, 0.0000]]]]],
                   grad_fn=<ViewBackward0>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...1659, 6.4108]],
            
                      [[8.2731, 7.0212]]]]], grad_fn=<RepeatBackward0>)
        )
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[[[ 0.4308, -0.5459,  0.37...        [ 0.7915, -0.2457, -0.2296, -0.0906,  0.6072, -0.8651]]]]],
               grad_fn=<RepeatBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: NYI: Tensor.clone(memory_format) inside vmap is only supported with memory_format torch.preserve_format or torch.contiguous_format (got ChannelsLast)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_______ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_prelu_cpu_float32 _______

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_prelu_cpu_float32>
device = 'cpu', dtype = torch.float32
op = UnaryUfuncInfo(name='nn.functional.prelu', ref=<function <lambda> at 0x7fa8e1f4eaf0>, aliases=(), variant_test_name=''...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
            for loop_out, batched_out in generator:
>               self.assertEqual(loop_out, batched_out)
E               AssertionError: Tensor-likes are not close!
E               
E               Mismatched elements: 50 / 50 (100.0%)
E               Greatest absolute difference: 22.793102264404297 at index (0, 2, 0) (up to 0.0001 allowed)
E               Greatest relative difference: inf at index (0, 0, 0) (up to 0.0001 allowed)
E               
E               The failure occurred for item [4]

test/test_ops.py:539: AssertionError
_______ TestOperatorsCPU.test_vmapvjpvjp_nn_functional_rrelu_cpu_float32 _______

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_nn_functional_rrelu_cpu_float32>
device = 'cpu', dtype = torch.float32
op = UnaryUfuncInfo(name='nn.functional.rrelu', ref=None, aliases=(), variant_test_name='', op=<function <lambda> at 0x7fa8...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:12913: in <lambda>
    wrapper_set_seed(torch.nn.functional.rrelu, input, *args, **kwargs),
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: in wrapper_set_seed
    return op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
  ....9730,  3.2708,
                     -3.8939,  2.8211, -4.7024,  4.1631]], grad_fn=<RepeatBackward0>)
        )
    )
)
lower = 0.0, upper = 1.0, training = True, inplace = False

    def rrelu(
        input: Tensor, lower: float = 1.0 / 8, upper: float = 1.0 / 3, training: bool = False, inplace: bool = False
    ) -> Tensor:
        r"""rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor
    
        Randomized leaky ReLU.
    
        See :class:`~torch.nn.RReLU` for more details.
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                rrelu, (input,), input, lower=lower, upper=upper, training=training, inplace=inplace
            )
        if inplace:
            result = torch.rrelu_(input, lower, upper, training)
        else:
>           result = torch.rrelu(input, lower, upper, training)
E           RuntimeError: vmap: we do not yet support aten::rrelu_with_noise. Please file an issue

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1682: RuntimeError
_____________ TestOperatorsCPU.test_vmapvjpvjp_normal_cpu_float32 ______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_normal_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='normal', ref=None, aliases=(), variant_test_name='', op=<function <lambda> at 0x7fa8e1e21b80>, method_var...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:15598: in <lambda>
    wrapper_set_seed(torch.normal, inp, *args, **kwargs),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op = <built-in method normal of type object at 0x7fab59eeb620>
args = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ... BatchedTensor(lvl=1, bdim=0, value=
            tensor([3.6171, 3.6171], grad_fn=<RepeatBackward0>)
        )
    )
))
kwargs = {}

    def wrapper_set_seed(op, *args, **kwargs):
        """Wrapper to set seed manually for some functions like dropout
        See: https://github.com/pytorch/pytorch/pull/62315#issuecomment-896143189 for more details.
        """
        with freeze_rng_state():
            torch.manual_seed(42)
>           return op(*args, **kwargs)
E           RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: RuntimeError
_______ TestOperatorsCPU.test_vmapvjpvjp_normal_number_mean_cpu_float32 ________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_normal_number_mean_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='normal', ref=None, aliases=(), variant_test_name='number_mean', op=<function <lambda> at 0x7fa8e1e27040>,...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:15619: in <lambda>
    wrapper_set_seed(torch.normal, mean, std, *args, **kwargs),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op = <built-in method normal of type object at 0x7fab59eeb620>
args = (0.3, GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, val...0.4662, 4.2133],
                     [6.0643, 2.9832, 7.0534, 5.0676]]], grad_fn=<RepeatBackward0>)
        )
    )
))
kwargs = {}

    def wrapper_set_seed(op, *args, **kwargs):
        """Wrapper to set seed manually for some functions like dropout
        See: https://github.com/pytorch/pytorch/pull/62315#issuecomment-896143189 for more details.
        """
        with freeze_rng_state():
            torch.manual_seed(42)
>           return op(*args, **kwargs)
E           RuntimeError: vmap: called random operation while in randomness error mode. Please either use the 'same' or 'different' randomness flags on vmap or perform the randomness operation out of vmap

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:8198: RuntimeError
______________ TestOperatorsCPU.test_vmapvjpvjp_prod_cpu_float32 _______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_prod_cpu_float32>
device = 'cpu', dtype = torch.float32
op = ReductionOpInfo(name='prod', ref=<function prod at 0x7fa8e1cf81f0>, aliases=(), variant_test_name='', op=<built-in met...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
            tensor([-0., -0.], grad_fn=<ProdBackward1>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...     [-0.9515,  0.3260,  0.9574, -0.7927, -0.2160]]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([0.8806, 0.8806], grad_fn=<RepeatBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: We do not support batching operators that can output dynamic shape. Attempted to vmap over aten::nonzero. Please voice your support in https://github.com/pytorch/functorch/issues/256

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_______________ TestOperatorsCPU.test_vmapvjpvjp_put_cpu_float32 _______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_put_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='put', ref=None, aliases=(), variant_test_name='', op=<built-in method put of type object at 0x7fab59eeb62...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:530: in vjp_of_vjp
    result_vjps = vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    tensor([[ 0.1021, -0.2590, -0.1549],
            [-1.3706, -0.1319,  0.8848],
 ...=True)
), GradTrackingTensor(lvl=-2, value=
    tensor([-0.2611, -0.1319, -0.1549], grad_fn=<ReshapeAliasBackward0>)
))
inputs = [GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=2, value=
        tensor([[[ 3.1285,  3.1285],
      ...90, -0.1549],
            [-1.3706, -0.1319,  0.8848],
            [-0.2611,  0.6104, -0.0098]], requires_grad=True)
)]
grad_outputs = (BatchedTensor(lvl=1, bdim=2, value=
    tensor([[[-1.4473, -1.4473],
             [-0.2039, -0.2039],
             [ ...l=1, bdim=1, value=
    tensor([[ 0.8184,  0.8184],
            [-0.7649, -0.7649],
            [ 0.8042,  0.8042]])
))
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: aten::put_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 1. Please try to use out-of-place operators instead of aten::put_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
____________ TestOperatorsCPU.test_vmapvjpvjp_quantile_cpu_float32 _____________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_quantile_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='quantile', ref=None, aliases=(), variant_test_name='', op=<built-in method quantile of type object at 0x7...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = OpInfo(name='quantile', ref=None, aliases=(), variant_test_name='', op=<built-in method quantile of type object at 0x7...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)
args = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...         tensor([[0.0290, 0.4019],
                    [0.0290, 0.4019]], grad_fn=<RepeatBackward0>)
        )
    )
))
kwargs = {}

    def __call__(self, *args, **kwargs):
        """Calls the function variant of the operator."""
>       return self.op(*args, **kwargs)
E       RuntimeError: Batching rule not implemented for aten::equal. We could not generate a fallback.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: RuntimeError
_______ TestOperatorsCPU.test_vmapvjpvjp_scatter_reduce_prod_cpu_float32 _______

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_scatter_reduce_prod_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='scatter_reduce', ref=None, aliases=(), variant_test_name='prod', op=<built-in method scatter_reduce of ty...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ... -3.7637e+01,  3.0534e+00, -5.4374e+01, -7.3755e+00]]],
                   grad_fn=<ViewBackward0>)
        )
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=0, value=
 ...-3.0014e+00,  1.0101e+00,  4.9624e+00, -7.1304e-01]]],
                   grad_fn=<RepeatBackward0>)
        )
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=0, value=
        tensor([[[-0.4115, -0.3398, -0.5563...,
                 [-1.0545, -2.1433, -0.5419, -0.4255,  1.8125]]],
               grad_fn=<RepeatBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: It looks like you're either (1) calling .item() on a Tensor or (2) attempting to use a Tensor in some data-dependent control flow or (3) encountering this error in PyTorch internals. For (1): we don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. For (2): If you're doing some control flow instead, we don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 . For (3): please file an issue.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
______________ TestOperatorsCPU.test_vmapvjpvjp_stft_cpu_float32 _______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_stft_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='stft', ref=None, aliases=(), variant_test_name='', op=<function stft at 0x7fa9a9b7d8b0>, method_variant=<...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: in __call__
    return self.op(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        BatchedTensor(lvl=1, bdim=1, value=
  ...               [-3.2840, -3.2840],
                    [ 1.2673,  1.2673]], grad_fn=<CloneBackward0>)
        )
    )
)
n_fft = 7, hop_length = None, win_length = None, window = None, center = False
pad_mode = 'reflect', normalized = False, onesided = None, return_complex = None

    def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,
             win_length: Optional[int] = None, window: Optional[Tensor] = None,
             center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,
             onesided: Optional[bool] = None,
             return_complex: Optional[bool] = None) -> Tensor:
        r"""Short-time Fourier transform (STFT).
    
        .. warning::
            From version 1.8.0, :attr:`return_complex` must always be given
            explicitly for real inputs and `return_complex=False` has been
            deprecated. Strongly prefer `return_complex=True` as in a future
            pytorch release, this function will only return complex tensors.
    
            Note that :func:`torch.view_as_real` can be used to recover a real
            tensor with an extra last dimension for real and imaginary components.
    
        The STFT computes the Fourier transform of short overlapping windows of the
        input. This giving frequency components of the signal as they change over
        time. The interface of this function is modeled after (but *not* a drop-in
        replacement for) librosa_ stft function.
    
        .. _librosa: https://librosa.org/doc/latest/generated/librosa.stft.html
    
        Ignoring the optional batch dimension, this method computes the following
        expression:
    
        .. math::
            X[\omega, m] = \sum_{k = 0}^{\text{win\_length-1}}%
                                \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ %
                                \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),
    
        where :math:`m` is the index of the sliding window, and :math:`\omega` is
        the frequency :math:`0 \leq \omega < \text{n\_fft}` for ``onesided=False``,
        or :math:`0 \leq \omega < \lfloor \text{n\_fft} / 2 \rfloor + 1` for ``onesided=True``.
    
        * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time
          sequences.
    
        * If :attr:`hop_length` is ``None`` (default), it is treated as equal to
          ``floor(n_fft / 4)``.
    
        * If :attr:`win_length` is ``None`` (default), it is treated as equal to
          :attr:`n_fft`.
    
        * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from
          :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is
          treated as if having :math:`1` everywhere in the window. If
          :math:`\text{win\_length} < \text{n\_fft}`, :attr:`window` will be padded on
          both sides to length :attr:`n_fft` before being applied.
    
        * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on
          both sides so that the :math:`t`-th frame is centered at time
          :math:`t \times \text{hop\_length}`. Otherwise, the :math:`t`-th frame
          begins at time  :math:`t \times \text{hop\_length}`.
    
        * :attr:`pad_mode` determines the padding method used on :attr:`input` when
          :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for
          all available options. Default is ``"reflect"``.
    
        * If :attr:`onesided` is ``True`` (default for real input), only values for
          :math:`\omega` in :math:`\left[0, 1, 2, \dots, \left\lfloor
          \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]` are returned because
          the real-to-complex Fourier transform satisfies the conjugate symmetry,
          i.e., :math:`X[m, \omega] = X[m, \text{n\_fft} - \omega]^*`.
          Note if the input or window tensors are complex, then :attr:`onesided`
          output is not possible.
    
        * If :attr:`normalized` is ``True`` (default is ``False``), the function
          returns the normalized STFT results, i.e., multiplied by :math:`(\text{frame\_length})^{-0.5}`.
    
        * If :attr:`return_complex` is ``True`` (default if input is complex), the
          return is a ``input.dim() + 1`` dimensional complex tensor. If ``False``,
          the output is a ``input.dim() + 2`` dimensional real tensor where the last
          dimension represents the real and imaginary components.
    
        Returns either a complex tensor of size :math:`(* \times N \times T)` if
        :attr:`return_complex` is true, or a real tensor of size :math:`(* \times N
        \times T \times 2)`. Where :math:`*` is the optional batch size of
        :attr:`input`, :math:`N` is the number of frequencies where STFT is applied
        and :math:`T` is the total number of frames used.
    
        .. warning::
          This function changed signature at version 0.4.1. Calling with the
          previous signature may cause error or return incorrect result.
    
        Args:
            input (Tensor): the input tensor
            n_fft (int): size of Fourier transform
            hop_length (int, optional): the distance between neighboring sliding window
                frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)
            win_length (int, optional): the size of window frame and STFT filter.
                Default: ``None``  (treated as equal to :attr:`n_fft`)
            window (Tensor, optional): the optional window function.
                Default: ``None`` (treated as window of all :math:`1` s)
            center (bool, optional): whether to pad :attr:`input` on both sides so
                that the :math:`t`-th frame is centered at time :math:`t \times \text{hop\_length}`.
                Default: ``True``
            pad_mode (str, optional): controls the padding method used when
                :attr:`center` is ``True``. Default: ``"reflect"``
            normalized (bool, optional): controls whether to return the normalized STFT results
                 Default: ``False``
            onesided (bool, optional): controls whether to return half of results to
                avoid redundancy for real inputs.
                Default: ``True`` for real :attr:`input` and :attr:`window`, ``False`` otherwise.
            return_complex (bool, optional): whether to return a complex tensor, or
                a real tensor with an extra last dimension for the real and
                imaginary components.
    
        Returns:
            Tensor: A tensor containing the STFT result with shape described above
    
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                stft, (input,), input, n_fft, hop_length=hop_length, win_length=win_length,
                window=window, center=center, pad_mode=pad_mode, normalized=normalized,
                onesided=onesided, return_complex=return_complex)
        # NOTE: Do not edit. This code will be removed once the forward-compatibility
        #       period is over for PR #73432
        if center:
            signal_dim = input.dim()
            extended_shape = [1] * (3 - signal_dim) + list(input.size())
            pad = int(n_fft // 2)
            input = F.pad(input.view(extended_shape), [pad, pad], pad_mode)
            input = input.view(input.shape[-signal_dim:])
>       return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
                        normalized, onesided, return_complex)
E       RuntimeError: vmap: Calling Tensor.as_strided is not supported unless the batch dims being vmapped over are at the front of the tensor (in memory layout). When they are not at the front of the tensor this operation can be error prone so we actively discourage it; please file us a bug report and/or try to express the as_strided operation in terms of PyTorch view operations

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/functional.py:606: RuntimeError
_______________ TestOperatorsCPU.test_vmapvjpvjp_svd_cpu_float32 _______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_svd_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='svd', ref=None, aliases=(), variant_test_name='', op=<built-in method svd of type object at 0x7fab59eeb62...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
            for loop_out, batched_out in generator:
>               self.assertEqual(loop_out, batched_out)
E               AssertionError: Tensor-likes are not close!
E               
E               Mismatched elements: 2 / 150 (1.3%)
E               Greatest absolute difference: 0.0008196830749511719 at index (1, 0, 4, 0) (up to 0.0001 allowed)
E               Greatest relative difference: 0.0001572327852315453 at index (1, 0, 3, 2) (up to 0.0001 allowed)
E               
E               The failure occurred for item [1]

test/test_ops.py:539: AssertionError
______________ TestOperatorsCPU.test_vmapvjpvjp_take_cpu_float32 _______________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_take_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='take', ref=None, aliases=(), variant_test_name='', op=<built-in method take of type object at 0x7fab59eeb...s=True, test_neg_view=True, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:224: in compute_quantities_for_vmap_test
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:349: in wrapped
    return _flat_vmap(
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/vmap.py:475: in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:228: in wrapped
    return vjp_fn(cotangents)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:304: in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:115: in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        tensor([-3.0337, -8.0676, -8.0676], grad_fn=<TakeBackward0>)
    )
),)
inputs = [GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        tensor([[-2.4005, -7.9506,  3.6116],
                [-8.0676, -0.5735,  3.1285],
                [-3.0337,  5.1067,  1.1351]], requires_grad=True)
    )
)]
grad_outputs = (GradTrackingTensor(lvl=-2, value=
    BatchedTensor(lvl=1, bdim=1, value=
        tensor([[ 0.2310,  0.2310],
                [ 0.6931,  0.6931],
                [-0.2669, -0.2669]], grad_fn=<CloneBackward0>)
    )
),)
retain_graph = True, create_graph = True, only_inputs = True
allow_unused = True, is_grads_batched = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False,
        is_grads_batched: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in vector-Jacobian product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).
            To accumulate gradient for other parts of the graph, please use
            ``torch.autograd.backward``.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the vector-Jacobian product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
            is_grads_batched (bool, optional): If ``True``, the first dimension of each
                tensor in ``grad_outputs`` will be interpreted as the batch dimension.
                Instead of computing a single vector-Jacobian product, we compute a
                batch of vector-Jacobian products for each "vector" in the batch.
                We use the vmap prototype feature as the backend to vectorize calls
                to the autograd engine so that this computation can be performed in a
                single call. This should lead to performance improvements when compared
                to manually looping and performing backward multiple times. Note that
                due to this feature being experimental, there may be performance
                cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``
                to show any performance warnings and file an issue on github if warnings exist
                for your use case. Defaults to ``False``.
        """
        t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))
        t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))
        overridable_args = t_outputs + t_inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                t_outputs,
                t_inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
                is_grads_batched=is_grads_batched,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))
        grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
    
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment several times below is because
        # some Python versions print out the first line of multi-line function
        # calls in the traceback and some print out the last line
        if is_grads_batched:
            def vjp(gO):
                return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                    t_outputs, gO, retain_graph, create_graph, t_inputs,
                    allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
            return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)
        else:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,
                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass
E           RuntimeError: vmap: aten::put_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 1. Please try to use out-of-place operators instead of aten::put_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: RuntimeError
_________ TestOperatorsCPU.test_vmapvjpvjp_view_as_complex_cpu_float32 _________

self = <test_ops.TestOperatorsCPU testMethod=test_vmapvjpvjp_view_as_complex_cpu_float32>
device = 'cpu', dtype = torch.float32
op = OpInfo(name='view_as_complex', ref=None, aliases=(), variant_test_name='', op=<built-in method view_as_complex of type...=True, test_neg_view=False, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)

    @ops(functorch_lagging_op_db + additional_op_db, allowed_dtypes=(torch.float,))
    @skipOps('TestOperators', 'test_vmapvjpvjp', vjp_fail.union({
        skip("atleast_1d"),  # Takes too long
        skip("atleast_2d"),  # Takes too long
        skip("atleast_3d"),  # Takes too long
        xfail("_masked.prod"),  # calls item
        xfail("as_strided"),  # incorrect output
        skip("bernoulli"),  # calls random op
        xfail("bfloat16"),  # rank 4 tensor for channels_last
        xfail("cumprod"),  # calls item
        xfail("double"),  # rank 4 tensor for channels_last
        xfail("eig"),  # calls item
        xfail("float"),  # rank 4 tensor for channels_last
        xfail("half"),  # rank 4 tensor for channels_last
    }))
    @toleranceOverride({torch.float32: tol(atol=1e-04, rtol=1e-04)})
    def test_vmapvjpvjp(self, device, dtype, op):
        # Since, we test `vjpvjp` independently,
        # for this test, we just verify that vmap
        # of `vjpvjp` is correct.
        if not op.supports_autograd:
            self.skipTest("Skipped! Autograd not supported.")
            return
        if not op.supports_gradgrad:
            self.skipTest("Skipped! Operation does not support gradgrad")
            return
    
        samples = op.sample_inputs(device, dtype, requires_grad=True)
    
        # TODO: test in-place
        if is_inplace(op, op.get_op()):
            self.skipTest("Skipped! NYI: inplace-testing not supported.")
            return
    
        for sample in samples:
            fn, args = get_vjpfull_variant(op, sample)
            result = fn(*args)
            cotangents = tree_map(lambda x: torch.randn_like(x), result)
            cotangents, _ = tree_flatten(cotangents)
            num_args = len(args)
    
            args_and_cotangents = tuple(args) + tuple(cotangents)
    
            def vjp_of_vjp(*args_and_cotangents):
                args = args_and_cotangents[:num_args]
                cotangents = args_and_cotangents[num_args:]
                result, vjp_fn = vjp(fn, *args)
                result_vjps = vjp_fn(cotangents)
                result, _ = tree_flatten(result)
                result_vjps, _ = tree_flatten(result_vjps)
                return (*result, *result_vjps)
    
            is_batch_norm_and_training = is_batch_norm_training(op.name, sample.kwargs)
            generator = get_fallback_and_vmap_exhaustive(
                vjp_of_vjp, args_and_cotangents, {}, is_batch_norm_and_training=is_batch_norm_and_training)
>           for loop_out, batched_out in generator:

test/test_ops.py:538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:254: in get_fallback_and_vmap_exhaustive
    for quantities in compute_quantities_for_vmap_test(
test/common_utils.py:214: in compute_quantities_for_vmap_test
    loop_out = loop(op, in_dims, out_dim, batch_size, *batched_args, **kwarg_values)
test/common_utils.py:29: in loop
    out = op(*pytree.tree_unflatten(new_args, args_spec), **kwarg_values)
test/test_ops.py:529: in vjp_of_vjp
    result, vjp_fn = vjp(fn, *args)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:224: in wrapped
    result, vjp_fn = vjp(fn, *primals)
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/functorch-0.3.0a0+6a09847-py3.8-linux-x86_64.egg/functorch/_src/eager_transforms.py:270: in vjp
    primals_out = func(*diff_primals)
test/test_ops.py:114: in wrapped
    result = f(*_args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = OpInfo(name='view_as_complex', ref=None, aliases=(), variant_test_name='', op=<built-in method view_as_complex of type...=True, test_neg_view=False, assert_jit_shape_analysis=False, supports_expanded_weight=False, is_factory_function=False)
args = (GradTrackingTensor(lvl=-2, value=
    GradTrackingTensor(lvl=-2, value=
        tensor([[-8.4784, -1.7658],
         ... 3.6116],
                [-8.0676, -0.5735],
                [ 3.1285, -3.0337]], grad_fn=<SelectBackward0>)
    )
),)
kwargs = {}

    def __call__(self, *args, **kwargs):
        """Calls the function variant of the operator."""
>       return self.op(*args, **kwargs)
E       RuntimeError: Tensor must have a last dimension with stride 1

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: RuntimeError
=============================== warnings summary ===============================
../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:91
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:91: UserWarning: PyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild PyTorch making sure the library is visible to the build system.
    warnings.warn(

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.floating_types() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64).
    warnings.warn(msg, FutureWarning)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.floating_types_and() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64, torch.float16, torch.bfloat16).
    warnings.warn(msg, FutureWarning)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.floating_types_and() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64, torch.bfloat16, torch.float16).
    warnings.warn(msg, FutureWarning)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.floating_types_and() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64, torch.float16).
    warnings.warn(msg, FutureWarning)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.all_types_and_complex_and() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64, torch.complex64, torch.complex128, torch.bool, torch.float16, torch.bfloat16).
    warnings.warn(msg, FutureWarning)

../../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.all_types_and_complex_and() is deprecated since 1.12 and will be removed in 1.14. This call can be replaced with (torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64, torch.complex64, torch.complex128, torch.bool, torch.float16, torch.bfloat16, torch.complex32).
    warnings.warn(msg, FutureWarning)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp___rpow___cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/_tensor.py:850: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    return torch.tensor(other, dtype=dtype, device=self.device) ** self

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cholesky_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
  L = torch.cholesky(A)
  should be replaced with
  L = torch.linalg.cholesky(A)
  and
  U = torch.cholesky(A, upper=True)
  should be replaced with
  U = torch.linalg.cholesky(A).mH().
  This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1805.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_combinations_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_select_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::masked_scatter. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_complex_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::complex. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_copysign_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::copysign.Tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cross_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cross_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_cross. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cross_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_cross_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_cross. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_cumprod_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_prod_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::any. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_eig_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
  torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
  L, _ = torch.eig(A)
  should be replaced with
  L_complex = torch.linalg.eigvals(A)
  and
  L, V = torch.eig(A, eigenvectors=True)
  should be replaced with
  L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:3366.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_eig_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::eig. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfft2_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfftn_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_conj_physical. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfft2_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_fft_ihfftn_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_conj_physical. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_copy_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_fill_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::index_fill.int_Scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_index_fill_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::index_fill.int_Scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_istft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: istft will require a complex-valued input tensor in a future PyTorch release. Matching the output from stft with return_complex=True.  (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:977.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_istft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::col2im. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_istft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::col2im_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_istft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::col2im. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_det_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigvals_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_slogdet_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_tensorsolve_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logdet_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_linalg_solve_ex. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_det_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigvals_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_slogdet_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_tensorsolve_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_logdet_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_solve_ex. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_householder_product_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::tril_. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_lstsq. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_grad_oriented_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_pinv.atol_rtol_tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_grad_oriented_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_methods_invocations.py:10875: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_lstsq. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    op=lambda a, b, driver: torch.linalg.lstsq(a, b, driver=driver)[0],

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lu_factor_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lu_factor_ex_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::lu_unpack. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_pinv_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_pinv_hermitian_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_pinverse_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_pinv.atol_rtol_tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_tensorsolve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_linalg_solve_ex. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/functional.py:1643: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
  LU, pivots = torch.lu(A, compute_pivots)
  should be replaced with
  LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
  and
  LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
  should be replaced with
  LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2104.)
    return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.lu_solve is deprecated in favor of torch.linalg.lu_solveand will be removed in a future PyTorch release.
  Note that torch.linalg.lu_solve has its arguments reversed.
  X = torch.lu_solve(B, LU, pivots)
  should be replaced with
  X = torch.linalg.lu_solve(LU, pivots, B) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2261.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_lu_solve. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::linalg_lu_solve. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_unpack_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::lu_unpack. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_fill_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::masked_fill.Tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_masked_scatter_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::masked_scatter. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanmean_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nansum_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::nansum. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanquantile_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_quantile_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::lerp_.Tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanquantile_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_quantile_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::all. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_bilinear_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_trilinear. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_bilinear_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_trilinear. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:884.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_dropout2d_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
    warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_gaussian_nll_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:2827: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::any. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    if torch.any(var < 0):

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_huber_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:3224: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::huber_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_huber_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::huber_loss_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_kl_div_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:2910: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
    warnings.warn(

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_logsigmoid_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [2, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_logsigmoid_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool3d_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:843: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_pool3d_with_indices. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool3d_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_pool3d_with_indices_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool3d_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:868: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_pool3d_with_indices. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool1d_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool1d_grad_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:948: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_unpool2d. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.max_unpool2d(input.unsqueeze(-1), indices.unsqueeze(-1), output_size).squeeze(-1)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_grad_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:980: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_unpool2d. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.max_unpool2d(input, indices, output_size)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool3d_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool3d_grad_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:1012: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::max_unpool3d. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_multilabel_soft_margin_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_multilabel_soft_margin_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: An output with one or more elements was resized since it had shape [5, 5], which does not match the required output shape [2, 5, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_smooth_l1_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:3194: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::smooth_l1_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_smooth_l1_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::smooth_l1_loss_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_soft_margin_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:3405: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::soft_margin_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_soft_margin_loss_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::soft_margin_loss_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_upsample_bilinear_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:4064: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
    warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_upsample_nearest_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/nn/functional.py:4008: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
    warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_put_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::put. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_put_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::take. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_put_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::put_. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_qr_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
  The boolean parameter 'some' has been replaced with a string parameter 'mode'.
  Q, R = torch.qr(A, some)
  should be replaced with
  Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2538.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_renorm_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::renorm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_amax_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1531.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_amax_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_amin_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_mean_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_prod_cpu_float32
test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_sum_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::scatter_reduce.two. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_prod_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::scatter_reduce.two. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_special_log_ndtr_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::special_log_ndtr. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_stft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/functional.py:606: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:800.)
    return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_stft_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::index_add_. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_symeig_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
  The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
  L, _ = torch.symeig(A, upper=upper)
  should be replaced with
  L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
  and
  L, V = torch.symeig(A, eigenvectors=True)
  should be replaced with
  L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2980.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::take. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_triangular_solve_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.
  torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.
  X = torch.triangular_solve(B, A).solution
  should be replaced with
  X = torch.linalg.solve_triangular(A, B). (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2304.)
    return self.op(*args, **kwargs)

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_unfold_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/autograd/__init__.py:294: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::unfold_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_vdot_cpu_float32
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/opinfo/core.py:910: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::vdot. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/kshiteej/Pytorch/pytorch/functorch/functorch/csrc/BatchedFallback.cpp:83.)
    return self.op(*args, **kwargs)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================= slowest 20 durations =============================
66.05s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_bilinear_cpu_float32
60.45s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_cpu_float32
52.95s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_meshgrid_list_of_tensors_cpu_float32
51.78s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_meshgrid_variadic_tensors_cpu_float32
28.08s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_solve_triangular_cpu_float32
23.98s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool1d_cpu_float32
23.53s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_multi_dot_cpu_float32
23.32s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_broadcast_tensors_cpu_float32
19.37s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_diff_cpu_float32
18.49s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_svd_cpu_float32
18.11s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_svd_cpu_float32
16.54s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool3d_cpu_float32
15.43s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_lstsq_grad_oriented_cpu_float32
11.97s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_cpu_float32
11.39s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_poisson_nll_loss_cpu_float32
9.58s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp__masked_norm_cpu_float32
8.70s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv_transpose3d_cpu_float32
8.47s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_dist_cpu_float32
8.13s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_lu_solve_cpu_float32
6.97s call     test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_addcdiv_cpu_float32
=========================== short test summary info ============================
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eig_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_eigvals_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_householder_product_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_linalg_svd_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nanquantile_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_batch_norm_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_binary_cross_entropy_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_depthwise_with_bias_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_conv2d_stride_groups_with_bias_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_dropout2d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_dropout_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_feature_alpha_dropout_with_train_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_fractional_max_pool2d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_fractional_max_pool3d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_gaussian_nll_loss_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_instance_norm_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_layer_norm_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_pool2d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_max_unpool2d_grad_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_prelu_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_nn_functional_rrelu_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_normal_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_normal_number_mean_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_prod_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_put_cpu_float32 - ...
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_quantile_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_scatter_reduce_prod_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_stft_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_svd_cpu_float32 - ...
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_take_cpu_float32
FAILED test/test_ops.py::TestOperatorsCPU::test_vmapvjpvjp_view_as_complex_cpu_float32
= 33 failed, 424 passed, 87 skipped, 13880 deselected, 11 xfailed, 120 warnings in 803.48s (0:13:23) =
