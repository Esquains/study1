{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.mlir.export\n",
    "\n",
    "This notebook shows an potential API for `torch_mlir`, modeled after `torch.onnx.export` and serving similar use cases. It is implemented as a pure-Python dependency that mostly provides a nice wrapper around `torch_mlir`. We probably want a function like this in `torch_mlir` anyway, but the real point under discussion here is how to get some \"official\" blessing of `torch_mlir` exported via the PyTorch `torch.mlir` Python module (or determining that it isn't even desirable to do that).\n",
    "\n",
    "Anush has a [PR](https://github.com/pytorch/pytorch/pull/65880) to add Torch-MLIR as a CMake `ExternalProject_Add`, but I didn't use that here because it feels like that is more for if we needed PyTorch to depend at the C++ level on Torch-MLIR. But it doesn't seem like we need that. It suffices to have a `torch_mlir` Python package built against the right PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYTHONPATH setup\n",
    "\n",
    "For this notebook, I've hardcoded PYTHONPATH to point at a `torch_mlir` that I built locally. So one of the big questions is how to deal with the Torch dependency on `torch_mlir`.\n",
    "\n",
    "This is one of the things that we could use the most guidance on -- how to best integrate with PT at the Python level. It seems like one good outcome would be that PyTorch's officially provided Torch/MLIR interop story (i.e. `torch.mlir` python package) would light up when the `torch_mlir` Python package is installed, but otherwise report a \"feature unavailable\" sort of error. **Would that be acceptable to PT devs?**\n",
    "\n",
    "Some topics adjacent to that:\n",
    "- What support matrix of {Python version}x{PyTorch version}x{OS}x{...} of packages do we need? (Presumably torch/xla and other PT-depending projects have crossed this path before... any pointers would be very welcome)\n",
    "- How to hook into the appropriate PT release channels? (if desirable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += [\"/usr/local/google/home/silvasean/pg/torch-mlir/torch-mlir/build/tools/torch-mlir/python_packages/torch_mlir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common setup\n",
    "\n",
    "This and the ONNX export is taken almost verbatim from https://pytorch.org/docs/master/onnx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(10, 3, 224, 224)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.onnx.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(10, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(1000, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(1000, strides=[1], requires_grad=1, device=cpu),\n",
      "      %193 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cpu),\n",
      "      %194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %196 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %197 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %199 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %200 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %202 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %203 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %205 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %206 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %208 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %209 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %211 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %212 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %214 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %215 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %217 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %218 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %220 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %221 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %223 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %224 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %226 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %227 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %229 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %230 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %232 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %233 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %235 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %236 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %238 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %239 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %241 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %242 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %244 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %245 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %247 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %248 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %250 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %251 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %192 : Float(10, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%input.1, %193, %194)\n",
      "  %125 : Float(10, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu(%192) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %126 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%125)\n",
      "  %195 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %196, %197)\n",
      "  %129 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%195) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %198 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%129, %199, %200)\n",
      "  %132 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add(%198, %126) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %133 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%132)\n",
      "  %201 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%133, %202, %203)\n",
      "  %136 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%201) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %204 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%136, %205, %206)\n",
      "  %139 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add(%204, %133) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %140 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%139) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %207 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%140, %208, %209)\n",
      "  %143 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%207) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %210 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%143, %211, %212)\n",
      "  %213 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%140, %214, %215)\n",
      "  %148 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add(%210, %213) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %149 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%148)\n",
      "  %216 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%149, %217, %218)\n",
      "  %152 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%216) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %219 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%152, %220, %221)\n",
      "  %155 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add(%219, %149) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %156 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%155) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %222 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%156, %223, %224)\n",
      "  %159 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%222) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %225 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%159, %226, %227)\n",
      "  %228 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%156, %229, %230)\n",
      "  %164 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add(%225, %228) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %165 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%164)\n",
      "  %231 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%165, %232, %233)\n",
      "  %168 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%231) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %234 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%168, %235, %236)\n",
      "  %171 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add(%234, %165) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %172 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%171) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %237 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%172, %238, %239)\n",
      "  %175 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%237) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %240 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%175, %241, %242)\n",
      "  %243 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%172, %244, %245)\n",
      "  %180 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%240, %243) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %181 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%180)\n",
      "  %246 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%181, %247, %248)\n",
      "  %184 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%246) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %249 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%184, %250, %251)\n",
      "  %187 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%249, %181) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:80:0\n",
      "  %188 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%187) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1345:0\n",
      "  %189 : Float(10, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%188) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1179:0\n",
      "  %190 : Float(10, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%189) # /usr/local/google/home/silvasean/pg/pytorch/dev.venv/lib/python3.9/site-packages/torchvision/models/resnet.py:243:0\n",
      "  %191 : Float(10, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%190, %fc.weight, %fc.bias) # /usr/local/google/home/silvasean/pg/pytorch/pytorch/torch/nn/functional.py:1896:0\n",
      "  return (%191)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, \"resnet.onnx\", verbose=True, export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.mlir.export\n",
    "\n",
    "This is the same almost-one-liner from the ONNX exporter (we are missing a bit of the sugar that `torch.onnx.export` provides, but it is easy to add that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module attributes {torch.debug_module_name = \"ResNet\"}  {\n",
      "  func @forward(%arg0: !torch.vtensor<[10,3,224,224],f32>) -> !torch.vtensor<[?,?],f32> {\n",
      "    %int-1 = torch.constant.int -1\n",
      "    %int3 = torch.constant.int 3\n",
      "    %true = torch.constant.bool true\n",
      "    %int0 = torch.constant.int 0\n",
      "    %float1.000000e-05 = torch.constant.float 1.000000e-05\n",
      "    %float1.000000e-01 = torch.constant.float 1.000000e-01\n",
      "    %int2 = torch.constant.int 2\n",
      "    %int1 = torch.constant.int 1\n",
      "    %0 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64x3x7x7xf32>) : !torch.vtensor<[64,3,7,7],f32>\n",
      "    %1 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %2 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %3 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %4 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %5 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>\n",
      "    %6 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %7 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %8 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %9 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %10 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>\n",
      "    %11 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %12 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %13 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %14 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %15 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>\n",
      "    %16 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %17 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %18 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %19 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %20 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>\n",
      "    %21 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %22 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %23 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %24 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<64xf32>) : !torch.vtensor<[64],f32>\n",
      "    %25 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128x64x3x3xf32>) : !torch.vtensor<[128,64,3,3],f32>\n",
      "    %26 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %27 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %28 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %29 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %30 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>\n",
      "    %31 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %32 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %33 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %34 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128x64x1x1xf32>) : !torch.vtensor<[128,64,1,1],f32>\n",
      "    %35 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %36 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %37 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %38 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %39 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>\n",
      "    %40 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %41 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %42 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %43 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %44 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>\n",
      "    %45 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %46 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %47 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %48 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<128xf32>) : !torch.vtensor<[128],f32>\n",
      "    %49 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256x128x3x3xf32>) : !torch.vtensor<[256,128,3,3],f32>\n",
      "    %50 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %51 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %52 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %53 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %54 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>\n",
      "    %55 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %56 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %57 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %58 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256x128x1x1xf32>) : !torch.vtensor<[256,128,1,1],f32>\n",
      "    %59 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %60 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %61 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %62 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %63 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>\n",
      "    %64 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %65 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %66 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %67 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %68 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>\n",
      "    %69 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %70 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %71 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %72 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<256xf32>) : !torch.vtensor<[256],f32>\n",
      "    %73 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512x256x3x3xf32>) : !torch.vtensor<[512,256,3,3],f32>\n",
      "    %74 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %75 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %76 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %77 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %78 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>\n",
      "    %79 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %80 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %81 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %82 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512x256x1x1xf32>) : !torch.vtensor<[512,256,1,1],f32>\n",
      "    %83 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %84 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %85 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %86 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %87 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>\n",
      "    %88 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %89 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %90 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %91 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %92 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>\n",
      "    %none = torch.constant.none\n",
      "    %false = torch.constant.bool false\n",
      "    %93 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %94 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %95 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %96 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<512xf32>) : !torch.vtensor<[512],f32>\n",
      "    %97 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<1000x512xf32>) : !torch.vtensor<[1000,512],f32>\n",
      "    %98 = torch.vtensor.literal(opaque<\"_\", \"0xDEADBEEF\"> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>\n",
      "    %99 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %100 = torch.prim.ListConstruct %int3, %int3 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %101 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %102 = torch.aten.conv2d %arg0, %0, %none, %99, %100, %101, %int1 : !torch.vtensor<[10,3,224,224],f32>, !torch.vtensor<[64,3,7,7],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %103 = torch.aten.batch_norm %102, %3, %4, %1, %2, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %104 = torch.aten.relu %103 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %105 = torch.prim.ListConstruct %int3, %int3 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %106 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %107 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %108 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %109 = torch.aten.max_pool2d %104, %105, %106, %107, %108, %false : !torch.vtensor<[?,?,?,?],f32>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %110 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %111 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %112 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %113 = torch.aten.conv2d %109, %5, %none, %110, %111, %112, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %114 = torch.aten.batch_norm %113, %8, %9, %6, %7, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %115 = torch.aten.relu %114 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %116 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %117 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %118 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %119 = torch.aten.conv2d %115, %10, %none, %116, %117, %118, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %120 = torch.aten.batch_norm %119, %13, %14, %11, %12, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %121 = torch.aten.add.Tensor %120, %109, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %122 = torch.aten.relu %121 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %123 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %124 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %125 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %126 = torch.aten.conv2d %122, %15, %none, %123, %124, %125, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %127 = torch.aten.batch_norm %126, %18, %19, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %128 = torch.aten.relu %127 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %129 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %130 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %131 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %132 = torch.aten.conv2d %128, %20, %none, %129, %130, %131, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %133 = torch.aten.batch_norm %132, %23, %24, %21, %22, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %134 = torch.aten.add.Tensor %133, %122, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %135 = torch.aten.relu %134 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %136 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %137 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %138 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %139 = torch.aten.conv2d %135, %25, %none, %136, %137, %138, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128,64,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %140 = torch.aten.batch_norm %139, %28, %29, %26, %27, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %141 = torch.aten.relu %140 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %142 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %143 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %144 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %145 = torch.aten.conv2d %141, %30, %none, %142, %143, %144, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %146 = torch.aten.batch_norm %145, %33, %38, %31, %32, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %147 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %148 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %149 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %150 = torch.aten.conv2d %135, %34, %none, %147, %148, %149, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128,64,1,1],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %151 = torch.aten.batch_norm %150, %37, %38, %35, %36, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %152 = torch.aten.add.Tensor %146, %151, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %153 = torch.aten.relu %152 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %154 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %155 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %156 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %157 = torch.aten.conv2d %153, %39, %none, %154, %155, %156, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %158 = torch.aten.batch_norm %157, %42, %43, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %159 = torch.aten.relu %158 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %160 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %161 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %162 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %163 = torch.aten.conv2d %159, %44, %none, %160, %161, %162, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %164 = torch.aten.batch_norm %163, %47, %48, %45, %46, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %165 = torch.aten.add.Tensor %164, %153, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %166 = torch.aten.relu %165 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %167 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %168 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %169 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %170 = torch.aten.conv2d %166, %49, %none, %167, %168, %169, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256,128,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %171 = torch.aten.batch_norm %170, %52, %53, %50, %51, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %172 = torch.aten.relu %171 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %173 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %174 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %175 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %176 = torch.aten.conv2d %172, %54, %none, %173, %174, %175, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %177 = torch.aten.batch_norm %176, %57, %62, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %178 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %179 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %180 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %181 = torch.aten.conv2d %166, %58, %none, %178, %179, %180, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256,128,1,1],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %182 = torch.aten.batch_norm %181, %61, %62, %59, %60, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %183 = torch.aten.add.Tensor %177, %182, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %184 = torch.aten.relu %183 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %185 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %186 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %187 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %188 = torch.aten.conv2d %184, %63, %none, %185, %186, %187, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %189 = torch.aten.batch_norm %188, %66, %67, %64, %65, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %190 = torch.aten.relu %189 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %191 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %192 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %193 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %194 = torch.aten.conv2d %190, %68, %none, %191, %192, %193, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %195 = torch.aten.batch_norm %194, %71, %72, %69, %70, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %196 = torch.aten.add.Tensor %195, %184, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %197 = torch.aten.relu %196 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %198 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %199 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %200 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %201 = torch.aten.conv2d %197, %73, %none, %198, %199, %200, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512,256,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %202 = torch.aten.batch_norm %201, %76, %77, %74, %75, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %203 = torch.aten.relu %202 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %204 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %205 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %206 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %207 = torch.aten.conv2d %203, %78, %none, %204, %205, %206, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %208 = torch.aten.batch_norm %207, %81, %86, %79, %80, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %209 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %210 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %211 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %212 = torch.aten.conv2d %197, %82, %none, %209, %210, %211, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512,256,1,1],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %213 = torch.aten.batch_norm %212, %85, %86, %83, %84, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %214 = torch.aten.add.Tensor %208, %213, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %215 = torch.aten.relu %214 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %216 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %217 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %218 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %219 = torch.aten.conv2d %215, %87, %none, %216, %217, %218, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %220 = torch.aten.batch_norm %219, %90, %91, %88, %89, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %221 = torch.aten.relu %220 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %222 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %223 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %224 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %225 = torch.aten.conv2d %221, %92, %none, %222, %223, %224, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.list<!torch.int>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %226 = torch.aten.batch_norm %225, %95, %96, %93, %94, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %227 = torch.aten.add.Tensor %226, %215, %int1 : !torch.vtensor<[?,?,?,?],f32>, !torch.vtensor<[?,?,?,?],f32>, !torch.int -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %228 = torch.aten.relu %227 : !torch.vtensor<[?,?,?,?],f32> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %229 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<!torch.int>\n",
      "    %230 = torch.aten.adaptive_avg_pool2d %228, %229 : !torch.vtensor<[?,?,?,?],f32>, !torch.list<!torch.int> -> !torch.vtensor<[?,?,?,?],f32>\n",
      "    %231 = torch.aten.flatten.using_ints %230, %int1, %int-1 : !torch.vtensor<[?,?,?,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[?,?],f32>\n",
      "    %232 = torch.aten.linear %231, %97, %98 : !torch.vtensor<[?,?],f32>, !torch.vtensor<[1000,512],f32>, !torch.vtensor<[1000],f32> -> !torch.vtensor<[?,?],f32>\n",
      "    return %232 : !torch.vtensor<[?,?],f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.mlir\n",
    "mlir_module = torch.mlir.export(torch.jit.script(model), [dummy_input])\n",
    "print(mlir_module.operation.get_asm(large_elements_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "ptdev",
   "language": "python",
   "name": "ptdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
