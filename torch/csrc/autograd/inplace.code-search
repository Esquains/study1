# Query: inplace
# Including: torch/csrc/autograd/*
# Excluding: *Type*, *factories*
# ContextLines: 1

156 results - 21 files

torch/csrc/autograd/autograd_meta.cpp:
   17  
   18: // [Forward Grad View/inplace]
   19: // It is important to us to allow view and inplace to work with dual Tensors.
   20  // These operations should either compute the right gradient or raise a

   52  //   forward grad.
   53: //   - Inplace operations must modify the input's forward grad inplace.
   54  //

  116  // This function is will ensure that the fw_grad_ is properly a view of the base
  117: // for inplace ops on Tensors that do not have forward grad originally.
  118  void AutogradMeta::set_fw_grad(

  121      uint64_t level,
  122:     bool is_inplace_op) {
  123    TORCH_CHECK(

  141      // Setting the forward grad again is only allowed if it is a no-op.
  142:     // We do allow this case to simplify writing codegen for inplace ops.
  143      TORCH_INTERNAL_ASSERT(

  148      TORCH_INTERNAL_ASSERT(
  149:         is_inplace_op,
  150:         "Only inplace operations can re-set the forward grad of a Tensor that "
  151          "already has one.");

  155          "Cannot set a value of a forward grad if it "
  156:         "already exists. Inplace operations should modify it inplace.");
  157    } else {

  172  
  173:     if (is_inplace_op && is_view_) {
  174        auto this_view_meta = static_cast<DifferentiableViewMeta*>(this);
  175  
  176:       // For inplace ops on a Tensor that does not already have a forward grad
  177        // and is a view, we propagate the tangent to the base and ensure that the
  178        // new_grad is a view of that base's tangent. This ensure that case 4 from
  179:       // [Forward Grad View/inplace] above works fine What happens in this long
  180        // if statement is:

  216  
  217:           base._set_fw_grad(new_base_fw_grad, level, /* is_inplace_op */ false);
  218          }

  257      // For view that don't have a forward grad, check if their base has one that
  258:     // has been defined by an inplace operation.
  259:     // This ensure that case 5 from [Forward Grad View/inplace] above works fine
  260      auto const_view_meta =

torch/csrc/autograd/autograd_not_implemented_fallback.cpp:
   65    // so we can rebase_history if necessary
   66:   std::vector<bool> is_inplace_output;
   67:   bool any_is_inplace_output = false;
   68    std::vector<bool> is_aliased_output;
   69:   is_inplace_output.reserve(num_returns);
   70    is_aliased_output.reserve(num_returns);

   73      const at::AliasInfo* alias_info = returns[i].alias_info();
   74:     is_inplace_output.push_back(alias_info != nullptr && alias_info->isWrite());
   75:     any_is_inplace_output |= alias_info != nullptr && alias_info->isWrite();
   76      is_aliased_output.push_back(alias_info != nullptr);

  125          if (alias_info != nullptr && alias_info->isWrite()) {
  126:           check_inplace(t, any_requires_grad);
  127          }

  159  #endif
  160:   if (aliased_input_idx != -1 || any_is_inplace_output) {
  161      at::AutoDispatchBelowAutograd guard;

  164      // If neither in-place nor view
  165:     at::AutoDispatchBelowADInplaceOrView guard;
  166      op.redispatchBoxed(
  167:         dispatch_keys & c10::after_ADInplaceOrView_keyset, stack);
  168    }

  187            return;
  188:         if (!is_inplace_output[idx_ret])
  189            TORCH_INTERNAL_ASSERT(

  229            if (isDifferentiableType(t.scalar_type())) {
  230:             if (is_inplace_output[idx_ret]) {
  231                // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)

  249  
  250: void autogradNotImplementedInplaceOrViewFallbackImpl(
  251      const c10::OperatorHandle& op,

  253      torch::jit::Stack* stack) {
  254:   // Mimics a subset of the logic from ADInplaceOrViewType kernel:
  255:   // - see gen_inplace_or_view_type.py
  256    // - this should only be used with autogradNotImplementedFallback above

  259    //
  260:   // NOTE [ Limitations of ADInplaceOrView boxed kernel ]
  261    //

  270    //   and the first output (which may be either Tensor or vec of Tensors
  271:   // - For inplace (TODO?): enforce that the same op cannot be both a view and
  272:   // inplace
  273:   //   that is not allowed in the gen_inplace_or_view logic
  274    const auto& schema = op.schema();

  289            aliased_output_idx == -1,
  290:           "Fallback ADInplaceOrView kernel expects only a single output in the operator schema to have a "
  291            "non-write alias annotation (i.e., 'Tensor(a)'). "

  304              aliased_input_idx == -1,
  305:             "Fallback ADInplaceOrView kernel expects only a single input in the operator schema to have a "
  306              "non-write alias annotation (i.e., 'Tensor(a)'). "

  320    }
  321:   // See NOTE [ Limitations of ADInplaceOrView boxed kernel ] above
  322    TORCH_CHECK(

  324            (aliased_input_idx == 0 && aliased_output_idx == 0),
  325:       "Fallback ADInplaceOrView kernel can only create view relationships between the first "
  326        "input and the first output (the output can be a vector of tensors). Please change the "

  330    {
  331:     at::AutoDispatchBelowADInplaceOrView guard;
  332      op.redispatchBoxed(
  333:         dispatch_keys & c10::after_ADInplaceOrView_keyset, stack);
  334    }

  349        // why we don't have to care about the view_func logic below.
  350:       // See NOTE [ View + Inplace detection ] for more details about this logic
  351        auto result = as_view(

  394  
  395: torch::CppFunction autogradNotImplementedInplaceOrViewFallback() {
  396    return torch::CppFunction::makeFromBoxedFunction<
  397:       &autogradNotImplementedInplaceOrViewFallbackImpl>();
  398  }

torch/csrc/autograd/autograd_not_implemented_fallback.h:
   9  
  10: TORCH_API torch::CppFunction autogradNotImplementedInplaceOrViewFallback();
  11  

torch/csrc/autograd/custom_function.cpp:
   30  //  1) Use the user-provided jvp function to populate the the outputs' forward
   31: //  gradient 2) Perform error checking to ensure that view and inplace ops are
   32  //  properly handled

   42  //    the output's base's forward grad must be the output's forward grad's base.
   43: //  - If an input was modified inplace (it must be an output as well) we make
   44  //  sure that its
   45: //    forward grad was also modified inplace and already present on the
   46  //    corresponding output.

   60  
   61:   // The tracking info below are used to perform the view and inplace checks.
   62    // They are lazily initialized to reduce the cost of this function in the

  157          // If there was already a forward grad for that input
  158:         // Just make sure that it is modified inplace and returned as-is
  159          TORCH_CHECK(
  160              out_grad._version() != grad_versions[inp_idx],
  161:             "An inplace custom Function is not modifying the "
  162:             "forward mode gradients inplace. If the forward is modifying an input inplace, then the jvp "
  163:             "function must modify the corresponding gradient inplace.")
  164          TORCH_CHECK(
  165              out_grad.unsafeGetTensorImpl() == grad_impls[inp_idx],
  166:             "An inplace custom Function is not returning the "
  167:             "forward mode gradients as-is. If the forward is modifying an input inplace, then the jvp "
  168:             "function must modify the gradient inplace and return it as-is.")
  169        } else {

  171          // one We could also use inputs[inp_idx] here as it is the same as out
  172:         out._set_fw_grad(out_grad, level, /* is_inplace_op */ true);
  173        }

  228              // the user returned one for the output To ensure that we maintain
  229:             // the view/inplace constraints, we consider this as an inplace op
  230              // This case CANNOT happen in codegen as all view ops are mapping

  232              // cannot have a forward grad if the base does not.
  233:             out._set_fw_grad(out_grad, level, /* is_inplace_op */ true);
  234              return;

  238  
  239:       out._set_fw_grad(out_grad, level, /* is_inplace_op */ false);
  240      }

  315            !(var.is_view() && num_outputs > 1),
  316:           "If your Function modifies inplace an input that is a view"
  317            " of another Tensor, your Function cannot return more than one Tensor. This is not supported"

  319            " .clone() for example) or make your Function only return one Tensor (potentially splitting"
  320:           " it into two Functions: one doing the inplace that returns a single Tensor and a second one"
  321            " that does the other operations). You can ask on the forum https://discuss.pytorch.org/ if"

  376      // (do not change the flag if we return and input that is a view as is). See
  377:     // NOTE [ View + Inplace detection ] for why we replace everything by a
  378      // warning.

  393    // If multiple differentiable outputs are returned, we do not allow views to
  394:   // be modified inplace See NOTE [ View + Inplace detection ] for more details
  395    if (num_diff_outputs > 1) {

  411          "Some elements marked as dirty during the forward method were not returned as output. The"
  412:         " inputs that are modified inplace must all be outputs of the Function.");
  413    }

torch/csrc/autograd/custom_function.h:
  65  ///      var.mul_(2);
  66: ///      // Mark var as modified by inplace operation
  67  ///      ctx->mark_dirty({var});

torch/csrc/autograd/FunctionsManual.cpp:
  1127  //
  1128: // This function does not support `self` derivatives for inplace functions.
  1129  //

  2003    if (isDefined(pos_weight)) {
  2004:     // pos_weight might need to be broadcasted, thus mul(target) is not inplace.
  2005      auto t = pos_weight->mul(target);

  5813  // Computes the jvp for `input * weight + bias` where weight and bias may be
  5814: // undefined Possibly modifies the input inplace
  5815  static Tensor _affine_jvp(

torch/csrc/autograd/input_buffer.cpp:
  78    if (old_var.is_sparse()) {
  79:     // It is safe to change the Tensor inplace if the Tensor is only used in
  80      // this buffer (this could be the gradient passed by the user) and that no

torch/csrc/autograd/python_function.cpp:
  343  // There is a considerable amount of complexity to handle if the operation
  344: // that produced these output tensors is inplace.  A mapping of *input*
  345  // tensors to variables (t2var) is used to test if this occurred, and

  624      PyObject* output_objects,
  625:     bool is_inplace,
  626      bool unpack_output) {

  630  
  631:   node->i_(jit::attr::inplace, is_inplace);
  632    if (PyObject* module_name = PyDict_GetItemString(

  706  
  707:   bool is_inplace = static_cast<bool>(grad_fn->dirty_tensors);
  708    _wrap_outputs(

  710    _trace_post_record(
  711:       node, op_obj, unpacked.input_vars, outputs, is_inplace, unpack_output);
  712  

torch/csrc/autograd/python_function.h:
  44      // out GIL!  When I forgot to do this by hand
  45:     // TestAutograd.test_inplace_view_python called me out about it.
  46      // If python is already dead, leak the wrapped python objects

  86    PyObject* non_differentiable;
  87:   // Python tuple of tensors which had inplace updates in the forward()
  88    // pass.  Set by Python with 'mark_dirty'.  If nullptr, no tensors were
  89:   // modified inplace.
  90    PyObject* dirty_tensors;

torch/csrc/autograd/python_variable_indexing.cpp:
  105    }
  106:   at::AutoDispatchBelowADInplaceOrView guard; // TODO: remove
  107    at::tracer::impl::NoTracerDispatchMode tracer_guard;

torch/csrc/autograd/python_variable.cpp:
   727    } else {
   728:     AutoDispatchBelowADInplaceOrView guard{}; // TODO: Remove.
   729      tracer::impl::NoTracerDispatchMode tracer_guard{};

  1077    if (names == Py_None) {
  1078:     at::internal_set_names_inplace(var, at::nullopt);
  1079    } else {

  1083          "names must either be None or a tuple of dim names");
  1084:     at::internal_set_names_inplace(var, torch::parseDimnameList(names));
  1085    }

torch/csrc/autograd/saved_variable.cpp:
   22      bool is_output,
   23:     bool is_inplace_on_view) {
   24    if (variable.defined()) {

   28      // If an inference tensor was saved for backward in an autograd session and
   29:     // then you reenter inference mode and make an inplace update to the tensor
   30      // without bumping version_counter, it'll lead to silent wrong result when

   52      is_output_ = is_output;
   53:     is_inplace_on_view_ = is_inplace_on_view;
   54  
   55:     if (is_inplace_on_view) {
   56        TORCH_INTERNAL_ASSERT(!is_leaf_ && is_output);

  122      bool is_output,
  123:     bool is_inplace_on_view)
  124      : SavedVariable(

  126            is_output,
  127:           is_inplace_on_view) {}
  128  

  140  
  141:   auto grad_fn = is_inplace_on_view_ ? weak_grad_fn_.lock()
  142        : !hooks_ ? saved_original_ ? data_.grad_fn() : nullptr

  160            << "one of the variables needed for gradient computation has been "
  161:              "modified by an inplace operation: ["
  162            << data_.toString() << " " << data_.sizes() << "]";

  222      auto new_fw_grad = fw_grad_->value(/* level */ 0);
  223:     var._set_fw_grad(new_fw_grad, /* level */ 0, /* is_inplace_op */ false);
  224    }

torch/csrc/autograd/saved_variable.h:
  28        bool is_output,
  29:       bool is_inplace_on_view = false);
  30    SavedVariable(

  32        bool is_output,
  33:       bool is_inplace_on_view = false);
  34    SavedVariable(SavedVariable&&) = default;

  58    // In that case, the grad_fn must be passed in to the unpack function when
  59:   // reconstructing the Variable (except when we are doing an inplace operation
  60    // on a view, see below). The field saved_orignal_ below reflects the two

  80    // Weak version of grad_fn_ that prevents leaks in rebase_history() for
  81:   // inplace views.
  82    // This variable is used when the user chooses to create a SavedVariable with
  83:   // is_inplace_on_view = true.
  84    // In that case, the grad_fn passed in to the unpack function at unwrapping

  91    bool was_default_constructed_ = true;
  92:   bool is_inplace_on_view_ = false;
  93    bool saved_original_ = false;

torch/csrc/autograd/variable.cpp:
   66    // not supported or the view function changes the metadata which is not
   67:   // recorded by as_strided See Note [View + Inplace update on base tensor] and
   68:   // [View + Inplace update on view tensor] for more details how we use this
   69    // function in backward.

   90          // When base is a view but doesn't carry a view_fn in
   91:         // DifferentiableViewMeta, it's a view that doesn't support inplace
   92          // update, e.g. unbind. In this case we should throw an error when
   93:         // inplace update happens in **forward**. One would naturally think the
   94          // following function will be first called in backward pass. But the
   95          // first call site is indeed in **forward** pass when we refresh
   96:         // `grad_fn` triggered by inplace update. Search Note [View + Inplace
   97          // update for view tensor] to for the call site.

  101                "This view is the output of a function that returns multiple views."
  102:               "Such functions do not allow the output views to be modified inplace."
  103:               "You should replace the inplace operation by an out-of-place one");
  104            return root_base;

  160    if (diff_view_meta && diff_view_meta->has_bw_view()) {
  161:     // See NOTE [ View + Inplace detection ]
  162      auto creation_meta = diff_view_meta->get_creation_meta();
  163:     // Do not use handle_view_on_rebase here as check_inplace should have been
  164      // called before this and either throw an error

  603    if (diff_view_meta && diff_view_meta->has_bw_view()) {
  604:     // See NOTE [ View + Inplace detection ]
  605      std::lock_guard<std::mutex> lock(diff_view_meta->mutex_);

  612        // This is an indirect rebase_history due to another view or the base
  613:       // being modified inplace
  614        handle_view_on_rebase(diff_view_meta, /* indirect */ true);
  615        TORCH_INTERNAL_ASSERT(diff_view_meta->output_nr_ == 0);
  616:       // Note [View + Inplace update for view tensor]
  617:       // An inplace update happened on Tensor `self` (which is a view).
  618        // For example:

  622        //   self = view_op_n(view_n-1)
  623:       //   self = inplace_op(self)
  624        //

  637        //
  638:       // See Note [View + Inplace update for base tensor] for what we do to base
  639        // tensor when an in-place operation happens.

  704      bool indirect) {
  705:   /// See NOTE [ View + Inplace detection ] for justification of the logic below
  706    auto creation_meta = diff_view_meta->get_creation_meta();

  724            modified_obj,
  725:           " modified inplace.");
  726      } else if (creation_meta == CreationMeta::INFERENCE_MODE) {

  729            modified_obj,
  730:           " modified inplace in normal mode.");
  731      } else {

  734            modified_obj,
  735:           " modified inplace with grad mode enabled.");
  736      }

  741            " This view is the output of a function that returns multiple views. Such functions do not"
  742:           " allow the output views to be modified inplace. You should replace the inplace operation by an"
  743            " out-of-place one.");

  748            " Given that this use case is ambiguous and error-prone, it is forbidden."
  749:           " You can clarify your code by moving both the view and the inplace either both"
  750:           " inside the no_grad block (if you don't want the inplace to be tracked) or both outside (if you want"
  751:           " the inplace to be tracked).");
  752      } else if (creation_meta == CreationMeta::INFERENCE_MODE) {

  756            " Given that this use case is ambiguous and error-prone, it is forbidden."
  757:           " You can clarify your code by moving both the view and the inplace either both"
  758:           " inside the inference_mode block (if you don't want the inplace to be tracked) or both outside (if you want"
  759:           " the inplace to be tracked).");
  760        TORCH_CHECK(false, msg);

  764            " This view was created inside a custom Function (or because an input was returned as-is) and the"
  765:           " autograd logic to handle view+inplace would override the custom backward associated with the custom"
  766            " Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by"

torch/csrc/autograd/variable.h:
  274        uint64_t level,
  275:       bool is_inplace_op) override;
  276  

  444  ///
  445: /// See Note [Forward Grad View/inplace] for more details on how we handle these
  446  /// hard cases.

  458  ///   + if a single autograd Node returns multiple differentiable views, if any
  459: ///     output is modified by an inplace operation, the autograd engine will
  460  ///     make an equivalent graph (corresponding to the view operations) without

  484  ///
  485: /// See Note [View + Inplace update for base tensor]
  486: /// and Note [View + Inplace update for view tensor] for the details how
  487: /// autograd handles inplace update with view ops.
  488  ///

  509  
  510: /// NOTE [ View + Inplace detection ]
  511  ///
  512: /// We want to detect views followed by inplace as they are often forbidden to
  513  /// ensure correctness of the computed gradients. But since we want to only

  515  /// view is created via the `make_variable_*_view()` functions. This tag is then
  516: /// checked by the `check_inplace()` function from `VariableTypeUtils.h` that
  517: /// should be called before every inplace operation and to detect cases where
  518  /// other views are modified and this one is rebased by side effect, we also

  558  /// Unified function to handle error checking when rebase happens
  559: /// indirect=true means that the caller is not doing the inplace, but the
  560: /// inplace happened somewhere else.
  561  TORCH_API void handle_view_on_rebase(

torch/csrc/autograd/functions/accumulate_grad.h:
  174          update_grad(std::move(result));
  175:       } else if (!at::inplaceIsVmapCompatible(variable_grad, new_grad)) {
  176          // Ideally we'd perform an in-place operation to avoid changing

torch/csrc/autograd/functions/comm.cpp:
  134    // Disable the autograd during the actual computation
  135:   // torch::cuda::gather does not return a view or change things inplace
  136    // so no need for extra logic here

torch/csrc/autograd/functions/tensor.h:
  22  
  23: // Note [View + Inplace update for base tensor]
  24  // Performs grad_view = fn(grad_view), but out-of-place.

  40  //   view_n = view_op_n(view_n-1)
  41: //   view_n = inplace_op(view_n)
  42  //

  71  //   1. We cannot afford keeping large tensors alive to recover views only.
  72: //   2. There are inplace checks when Tensors are loaded back to make sure
  73  //      they haven't been changed (including size metadata).

  82  
  83: // See Note [View + Inplace update for view tensor] for what we do to view
  84  // tensor when an in-place operation happens.

  96    // view and view_fn are redundant and view_fn will be used if available.
  97:   // See Note [View + Inplace update for base tensor] for details.
  98    at::TensorGeometry view;

torch/csrc/autograd/generated/Functions.cpp:
  326    if (should_compute_output({ self_ix })) {
  327:     auto grad_result = not_implemented("inplace version of acosh");
  328      copy_range(grad_inputs, self_ix, grad_result);

  353    if (should_compute_output({ self_ix })) {
  354:     auto grad_result = not_implemented("inplace version of asinh");
  355      copy_range(grad_inputs, self_ix, grad_result);

  380    if (should_compute_output({ self_ix })) {
  381:     auto grad_result = not_implemented("inplace version of atanh");
  382      copy_range(grad_inputs, self_ix, grad_result);

torch/csrc/autograd/generated/python_enum_tag.cpp:
  12          .value("dynamic_output_shape", at::Tag::dynamic_output_shape)
  13:         .value("inplace_view", at::Tag::inplace_view)
  14          .value("generated", at::Tag::generated)

torch/csrc/autograd/generated/python_variable_methods.cpp:
  1397    if (functorch_tls) {
  1398:     functorch_tls->checkSupportsInplaceRequiresGrad();
  1399    }
