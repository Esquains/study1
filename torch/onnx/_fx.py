# Owner(s): ["module: onnx"]
import copy
from typing import Callable, Dict

import functorch
import onnx

import torch
import torch._C
import torch._decomp
import torch._dynamo
import torch._ops
from torch.fx.passes.fake_tensor_prop import FakeTensorProp
from torch.onnx._globals import GLOBALS as ONNX_GLOBALS
from torch.onnx._internal import registration


def _create_op_overload_to_exporter_key_table() -> Dict[torch._ops.OpOverload, str]:
    table: Dict[torch._ops.OpOverload, str] = {}

    for attr_name in dir(torch.ops.aten):
        op_overload_packet = getattr(torch.ops.aten, attr_name)
        if not isinstance(op_overload_packet, torch._ops.OpOverloadPacket):
            continue
        for overload_name in op_overload_packet.overloads():
            op_overload = getattr(op_overload_packet, overload_name)
            # This line maps torch.ops.aten.add.Tensor, torch.ops.aten.add.Scalar, torch.ops.aten.add.out, etc
            # to "aten::add". This means the exporter for "aten::add" is used for all overloads of "aten::add".
            # This is applied to all ops under torch.ops.aten.
            #
            # TODO(wechi): in the future, we might want to write individual exporter for each overload, if,
            # for example, they have different type promotion rules. If so, just map different overloads to
            # different exporter keys.
            table[op_overload] = op_overload_packet._qualified_op_name

    return table


_op_overload_to_exporter_key_table = _create_op_overload_to_exporter_key_table()


def _create_onnx_friendly_decomposition_table() -> Dict[
    torch._ops.OpOverload, Callable
]:
    decomposition_table: Dict[torch._ops.OpOverload, Callable] = {}
    for op_overload, decomp_fn in torch._decomp.decomposition_table.items():
        # Skip decomposition into "prim::*" ops, because they are not generally supported by ONNX.
        # Skip decomposition for op_overload as long as that op_overload has a corresponding ONNX exporter.
        if (
            "torch._refs" in decomp_fn.__module__
            or op_overload in _op_overload_to_exporter_key_table
        ):
            continue
        decomposition_table[op_overload] = decomp_fn
    return decomposition_table


_onnx_friendly_decomposition_table = _create_onnx_friendly_decomposition_table()


def _retrieve_or_wrap_scalar_as_constant(g, fx_node_arg, fx_name_to_ts_value, dtype):
    """Map FX value to TorchScript value.

    When creating TorchScript graph from FX graph, we need a mapping from FX variable
    to TorchScript variable. This function maps FX variable, fx_node_arg, to torch.jit.Value.
    """

    ts_value = fx_node_arg
    if isinstance(ts_value, torch.fx.Node):
        # 1. fx_node_arg is a torch.fx.Node, which means
        #    fx_node_arg stands for the output of that torch.fx.Node.
        # 2. fx_node_arg (variable in torch.fx.Graph) is be mapped to
        #    torch.jit.Value, fx_name_to_ts_value[fx_node_arg.name],
        #    in TorchScript graph.
        ts_value = fx_name_to_ts_value[ts_value.name]
    elif isinstance(ts_value, float) or isinstance(ts_value, int):
        # Always promote scalar to tensor with element type "dtype."
        # Usually, "dtype" is extracted from the expected output tensor of the node.
        # If this assumption is broken, we probably need to
        #  1. add "scalar" type in ONNX  and extend all exporters to support it, or
        #  2. write type promotion logic for each operator.
        # TODO(wechi): the called exporting function should tell all allowed input and output types.
        # Then, here we can try type-casting if type-mismatch happens.
        ts_value = g.op("Constant", value_t=torch.tensor(ts_value, dtype=dtype))
    else:
        raise RuntimeError(f"Unexpected type of fx_node_arg: {type(fx_node_arg)}")
    return ts_value


def _wrap_fx_args_as_ts_args(g, node, fx_name_to_ts_value):
    """Map all FX arguments of a node to arguments in TorchScript graph."""

    # This function assumes the order of arguments in FX op is the
    # same as the order of arguments in TorchScript op.
    return tuple(
        _retrieve_or_wrap_scalar_as_constant(
            # The node.meta["val"] is generated by FakeTensorProp.
            g,
            arg,
            fx_name_to_ts_value,
            node.meta["val"].dtype,
        )
        for arg in node.args
    )


def _export_fx_to_ts(fx_module_with_metadata):
    # TODO(wechi): To get rid of TorchScript dependency,
    # "g" should just be onnx.GraphProto or an equivalent
    # data structure in ONNXScript.
    g = torch._C.Graph()
    fx_name_to_ts_value = {}
    for node in fx_module_with_metadata.graph.nodes:
        if node.op == "placeholder":
            # Input of graph.
            v = g.addInput(node.name)
            v.setType(torch._C.TensorType.create_from_tensor(node.meta["val"]))
            fx_name_to_ts_value[node.name] = v
        elif node.op == "call_function":
            # aten ops and other statless functions.
            if (
                isinstance(node.target, torch._ops.OpOverload)
                and node.target in _op_overload_to_exporter_key_table
            ):
                exporter_key = _op_overload_to_exporter_key_table[node.target]
                symbolic_function_group = registration.registry.get_function_group(
                    exporter_key
                )
                if symbolic_function_group is None:
                    raise ValueError(
                        "No symbolic function (ONNX exporter) group for {}".format(
                            exporter_key
                        )
                    )
                symbolic_fn = symbolic_function_group.get(14)
                if symbolic_fn is None:
                    raise ValueError(
                        "No symbolic function (ONNX exporter) for {} in opset {}".format(
                            exporter_key, 14
                        )
                    )
                ts_args = _wrap_fx_args_as_ts_args(g, node, fx_name_to_ts_value)
                v = symbolic_fn(g, *ts_args)
                fx_name_to_ts_value[node.name] = v
            else:
                raise RuntimeError(
                    "Unknown call_function target: {}".format(node.target)
                )
        elif node.op == "output":
            for arg in node.args[0]:
                g.registerOutput(fx_name_to_ts_value[arg.name])
        elif node.op == "call_method":
            # TODO(wechi): Support call_method.
            raise RuntimeError("call_method is not supported yet.")
        elif node.op == "call_module":
            # TODO(wechi): Support call_module.
            raise RuntimeError("call_module is not supported yet.")
        elif node.op == "get_attr":
            # TODO(wechi): Support get_attr.
            raise RuntimeError("get_attr is not supported yet.")
        else:
            # TODO(wechi): Support get_attr, call_module, call_method.
            raise RuntimeError("Found node type not defined in torch.fx: " + node.op)
    return g


def _ts_graph_to_onnx_model_in_protobuf(ts_graph):
    proto, _, _, _ = ts_graph._export_onnx(
        {},
        ONNX_GLOBALS.export_onnx_opset_version,
        {},
        False,
        torch.onnx.OperatorExportTypes.ONNX,
        False,
        False,
        {},
        True,
        "",
        {},
    )

    return proto


def _export(
    module: torch.fx.GraphModule,
    *args,
    decomposition_table: Dict[torch._ops.OpOverload, Callable] = None,
    use_binary_format: bool = True,
):
    # Export FX graph to ONNX ModelProto.
    if decomposition_table is None:
        # Use default decomposition table.
        decomposition_table = torch._decomp.decomposition_table
    # Apply decomposition table to the input graph.
    decomposed_module = functorch.make_fx(
        module, decomposition_table, tracing_mode="fake"
    )(*args)
    # Assign output types and shapes to each node.
    # TODO(wechi): It's possible to get symbolic types (and shapes)
    # for each node's output. Consider to set "tracing_mode=symbolic"
    # when calling make_fx and then remove FakeTensorProp below.
    FakeTensorProp(decomposed_module).propagate(*args)
    ts_graph = _export_fx_to_ts(decomposed_module)
    # Export TorchScript graph to ONNX ModelProto.
    onnx_model = _ts_graph_to_onnx_model_in_protobuf(ts_graph)
    if use_binary_format:
        # Return ModelProto in binary format.
        return onnx_model
    # Return ModelProto in readable format (printable).
    return onnx.ModelProto.FromString(onnx_model)


def _export_function(fn: Callable, *args, use_binary_format: bool = True):
    # args will be converted to symbolic tensor.
    args = copy.deepcopy(args)
    # Translate callable to FX graph.
    graph_module = functorch.make_fx(fn)(*args)
    # Export FX graph to ONNX ModelProto.
    return _export(
        graph_module,
        *args,
        decomposition_table=_onnx_friendly_decomposition_table,
        use_binary_format=use_binary_format,
    )


def _export_module(module: torch.nn.Module, *args, use_binary_format: bool = True):
    # args will be converted to symbolic tensor.
    args = copy.deepcopy(args)
    # Convert nn.Module to FX graph
    # TODO(wechi): There are several symbolic tracing mechanisms to convert
    # nn.Module to FX graph. We should choose the right one after they are
    # matured.
    graph_module, graph_guard = torch._dynamo.export(module, *args, aten_graph=True)
    # Export FX graph to ONNX ModelProto.
    return _export(
        graph_module,
        *args,
        decomposition_table=_onnx_friendly_decomposition_table,
        use_binary_format=use_binary_format,
    )
