diff --git a/aten/src/ATen/templates/aten_xla_type_default.cpp b/aten/src/ATen/templates/aten_xla_type_default.cpp
index 040a752156..657caa3dac 100644
--- a/aten/src/ATen/templates/aten_xla_type_default.cpp
+++ b/aten/src/ATen/templates/aten_xla_type_default.cpp
@@ -1,16 +1,17 @@
 // ${generated_comment}
-#include <torch_xla/csrc/aten_xla_type_default.h>
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type_default.h>
 
 #include <ATen/Context.h>
 #include <torch/library.h>
 #include <ATen/CPUGeneratorImpl.h>
 
-#include <tensorflow/compiler/xla/xla_client/debug_macros.h>
-#include <tensorflow/compiler/xla/xla_client/metrics.h>
-#include <tensorflow/compiler/xla/xla_client/tf_logging.h>
-#include <torch_xla/csrc/aten_xla_bridge.h>
-#include <torch_xla/csrc/XLANativeFunctions.h>
-#include <torch_xla/csrc/function_call_tracker.h>
+#include <lazy_tensors/computation_client/debug_macros.h>
+#include <lazy_tensors/computation_client/ltc_logging.h>
+#include <lazy_tensors/computation_client/metrics.h>
+#include <lazy_tensor_core/csrc/aten_ltc_bridge.h>
+#include <lazy_tensor_core/csrc/ts_backend/XLANativeFunctions.h>
+#include <lazy_tensor_core/csrc/ts_backend/ts_computation_client.h>
+#include <lazy_tensor_core/csrc/function_call_tracker.h>
 
 namespace ${cpp_namespace} {
 
@@ -54,10 +55,25 @@ std::vector<at::Tensor> to_device_opt(const std::vector<at::Tensor>& tensors, c1
     return output_tensors;
 }
 
+namespace {
+
+std::vector<at::Tensor> _to_eager(at::TensorList tensors) {
+    if (lazy_tensors::compiler::TSComputationClient::HardwareDeviceType() == at::kCUDA) {
+        std::vector<at::Tensor> cuda_tensors;
+        for (const auto& t : tensors) {
+            cuda_tensors.push_back(t.cuda());
+        }
+        return cuda_tensors;
+    }
+    return at::_to_cpu(tensors);
+}
+
+}  // namespace
+
 // convenience helper for converting tensors to cpu
 
-std::vector<at::Tensor> to_cpu(const at::TensorList& tensors) {
-    // We can't just call at::to_cpu() on the entire list of Tensors
+std::vector<at::Tensor> to_eager(const at::TensorList& tensors) {
+    // We can't just call at::to_eager() on the entire list of Tensors
     // Because it will break on undefined tensors. Separate out undefined tensors first.
     std::vector<at::Tensor> cpu_tensors(tensors.size());
     std::vector<at::Tensor> valid_tensors;
@@ -71,7 +87,7 @@ std::vector<at::Tensor> to_cpu(const at::TensorList& tensors) {
             cpu_tensors[i] = tensor;
         }
     }
-    auto cpu_valid_tensors = at::_to_cpu(valid_tensors);
+    auto cpu_valid_tensors = _to_eager(valid_tensors);
     for (size_t i = 0, defined_pos = 0; i < tensors.size(); ++i) {
         if (to_translate[i]) {
             cpu_tensors[i] = std::move(cpu_valid_tensors[defined_pos++]);
@@ -80,7 +96,7 @@ std::vector<at::Tensor> to_cpu(const at::TensorList& tensors) {
   return cpu_tensors;
 }
 
-std::vector<c10::optional<at::Tensor>> to_cpu(const std::vector<c10::optional<at::Tensor>>& tensors) {
+std::vector<c10::optional<at::Tensor>> to_eager(const std::vector<c10::optional<at::Tensor>>& tensors) {
     std::vector<c10::optional<at::Tensor>> opt_tensors(tensors.size());
     std::vector<at::Tensor> materialized_tensors;
     std::vector<bool> to_translate(tensors.size());
@@ -91,7 +107,7 @@ std::vector<c10::optional<at::Tensor>> to_cpu(const std::vector<c10::optional<at
             materialized_tensors.push_back(*tensor);
         }
     }
-    auto aten_materialized_tensors = to_cpu(materialized_tensors);
+    auto aten_materialized_tensors = to_eager(materialized_tensors);
     for (size_t i = 0, defined_pos = 0; i < tensors.size(); ++i) {
         if (to_translate[i]) {
           opt_tensors[i] =
diff --git a/tools/codegen/dest/gen_external_aten_fallbacks.py b/tools/codegen/dest/gen_external_aten_fallbacks.py
index 62fdd800b3..a147184eab 100644
--- a/tools/codegen/dest/gen_external_aten_fallbacks.py
+++ b/tools/codegen/dest/gen_external_aten_fallbacks.py
@@ -30,40 +30,7 @@ _FN_DENYLIST_REGEX = [
 # Instead, the codegen will figure out which ops to generate _out wrappers for
 # entirely from the yaml. Maintaining the same behavior as current XLA codegen for now.
 _FN_OUT = [
-    'abs',
     'add',
-    'acos',
-    'acosh',
-    'asin',
-    'asinh',
-    'atan',
-    'atan2',
-    'atanh',
-    'baddbmm',
-    'bernoulli',
-    'binary_cross_entropy',
-    'binary_cross_entropy_backward',
-    'clamp',
-    'div',
-    'gather',
-    'ger',
-    'hardsigmoid',
-    'kthvalue',
-    'index_select',
-    'inverse',
-    'log',
-    'masked_select',
-    'maximum',
-    'minimum',
-    'pow',
-    'prod',
-    'nonzero',
-    'round',
-    'normal',
-    'std',
-    'take',
-    'topk',
-    'var',
 ]
 
 # See Note [Auto generated composite kernels]
@@ -190,7 +157,7 @@ class GenExternalAtenFallback:
 
             tensorlist_intermediates_str = ''
             if len(tensorlist_args) > 0:
-                tensorlist_intermediates_str = '\n'.join([f'  auto {updated_name} = to_cpu({arg.name});'
+                tensorlist_intermediates_str = '\n'.join([f'  auto {updated_name} = to_eager({arg.name});'
                                                           for arg, updated_name in tensorlist_args.items()])
 
             opt_tensor_intermediates_str = ''
@@ -199,14 +166,14 @@ class GenExternalAtenFallback:
                 opt_tensor_intermediates_str = \
                     f'\n  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {{{arg_str}}};'
                 opt_tensor_intermediates_str += \
-                    '\n  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);'
+                    '\n  auto external_tensors_opt = to_eager(external_tensors_opt_tensors);'
 
             intermediates = ''
             if tensorlist_intermediates_str != '':
                 intermediates += tensorlist_intermediates_str + '\n'
             intermediates += \
                 f"  std::vector<at::Tensor> external_tensors_tensors = {{{', '.join([a.name for a in tensor_args.keys()])}}};"
-            intermediates += "\n  auto external_tensors = to_cpu(external_tensors_tensors);"
+            intermediates += "\n  auto external_tensors = to_eager(external_tensors_tensors);"
             if opt_tensor_intermediates_str != '':
                 intermediates += opt_tensor_intermediates_str
 
@@ -266,9 +233,9 @@ class GenExternalAtenFallback:
 
             return f"""\
 {dispatcher_sig.defn(name=func_name)} {{
-  XLA_FN_TRACK(3);
-  XLA_COUNTER("aten::{name}", 1);
-  TF_VLOG(3) << "XLA {name} :"{print_args_str};
+  LTC_FN_TRACK(3);
+  LTC_COUNTER("aten::{name}", 1);
+  LTC_VLOG(3) << "XLA {name} :"{print_args_str};
 {intermediates}
   {at_call}{collect_mutated_tensors}{update_tensors}{avoid_warning}{return_str}
 }}
diff --git a/tools/codegen/gen_backend_stubs.py b/tools/codegen/gen_backend_stubs.py
index a3b9ac254d..ce426f1b28 100644
--- a/tools/codegen/gen_backend_stubs.py
+++ b/tools/codegen/gen_backend_stubs.py
@@ -174,7 +174,7 @@ def run(source_yaml: str, output_dir: str, dry_run: bool) -> None:
                 'extra_cuda_headers': '',
                 'legacy_th_headers': '',
                 'external_backend_headers': f'''#include "{output_dir}/{backend_key}NativeFunctions.h"
-#include <torch_xla/csrc/aten_xla_type_default.h>''',
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type_default.h>''',
                 'namespaced_headers': '',
                 'DispatchKey': dispatch_key,
                 'dispatch_namespace': dispatch_key.lower(),
